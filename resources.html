---
layout: page
title: Resources
permalink: /resources
---

<!-- Add these tiles to your software.html page -->
<!-- ...existing code... -->


<!-- ...existing code... -->

<!-- Data Section -->
<section id="data" style="margin-top: 4rem;">
  <h2>Data</h2>
  <section class="selected-publications">
    <h2 class="section-title">Datasets</h2>
    <div class="pub-list">

      <!-- NTSEBENCH -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/NTSEBench_poster_placeholder.png" alt="NTSEBENCH thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/pdf/2407.10380" target="_blank" rel="noopener">NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models</a>
          </h3>
          <div class="pub-meta">
            <span class="authors">Pranshu Pandya, Vatsal Gupta, Agney S Talwarr, Tushar Kataria, Dan Roth, Vivek Gupta</span>
            <span class="venue"><a href="https://2025.naacl.org/">NAACL 2025</a></span>
          </div>
          <p class="pub-abstract">
            NTSEBENCH is a benchmark dataset of 2,728 multiple-choice questions with 4,642 images across 26 categories, sourced from India’s nationwide NTSE exam.
It evaluates cognitive multimodal reasoning skills, covering puzzles, series, analogies, and aptitude tasks beyond rote learning.
          </p>
          <div class="pub-links">
            <a class="btn btn-paper" href="https://github.com/NTSEBench/NTSEBench">data</a>
          </div>
        </div>
      </article>


      <!-- TempTabQA -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/Enhancing_temporal_poster_placeholder.png" alt="TempTabQA thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2203.01916" target="_blank" rel="noopener">TempTabQA: Temporal Table Question Answering</a>
          </h3>
<div class="pub-meta">
            <span class="authors">Irwin Deng, Kushagra Dixit, Dan Roth, Vivek Gupta</span>
            <span class="venue"> <a href="https://2025.naacl.org/" >NAACL 2025</a></span>
          </div>
          <p class="pub-abstract">
           TempTabQA is a dataset of 11,454 question–answer pairs from 1,208 Wikipedia Infobox tables across 90+ domains, designed to test temporal reasoning in semi-structured data.
Evaluations show that leading NLP models lag human performance by over 13.5 F1 points.
It serves as a benchmark to advance models’ ability to handle temporal information in complex data structures.
          </p>
          <div class="pub-links">
            <a class="btn btn-code" href="https://github.com/temptabqa/temptabqa">data</a>
          </div>
        </div>
      </article>

      <!-- MapWise -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/MapWise_poster_placeholder.png" alt="MapWise thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2305.12345" target="_blank" rel="noopener">MapWise: Spatial Reasoning over Maps</a>
          </h3>
          <div class="pub-meta">
            <span class="authors">Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Manish Shrivastava, Dan Roth, Vivek Gupta</span>
            <span class="venue"> <a href="https://2025.naacl.org/" >NAACL 2025</a></span>
          </div>
          <p class="pub-abstract">
            MAPWise is a multimodal benchmark comprising 3,000 manually annotated question–answer pairs (1,000 per country) derived from official statistics of India, the USA, and China.
The maps are generated in multiple formats—2 legend types (discrete, continuous), 2 annotation settings (with, without), hatching textures, 3 colormap variations, and 2 background colors—systematically varying visual complexity.
Questions span 3 types (binary, direct value extraction, region association) and 6 answer formats (Yes/No, single word, count, list, range, ranking), enabling fine-grained statistical evaluation of models’ numerical, categorical, and geospatial reasoning performance.
          <div class="pub-links">
            <a class="btn btn-code" href="https://github.com/abhishekrajgaria/Mapwise/tree/main/dataset">data</a>
          </div>
        </div>
      </article>
<!-- TempTabQA-->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/temptabqa-c.png" alt="TEMPTABQA-C thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2106.12345" target="_blank" rel="noopener">TempTabQA-C</a>
          </h3>
          <div class="pub-meta">
            <span class="authors">Irwin Deng, Kushagra Dixit, Dan Roth, Vivek Gupta</span>
            <span class="venue"> <a href="https://2025.naacl.org/" >NAACL 2025</a></span>
          </div>

          <p class="pub-abstract">TEMPTABQA-C is a controlled benchmark of 200,000 question–answer pairs generated from Wikipedia infoboxes, stored in a relational schema to test temporal tabular reasoning.
Questions are annotated along three axes: Original vs. Counterfactual (fact perturbations), Small vs. Large tables, and Easy → Hard reasoning difficulty.
The dataset supports evaluation of direct prompting versus symbolic SQL generation, showing SQL integration significantly improves robustness to fact changes, table size, and reasoning complexity.TEMPTABQA-C is a large-scale, semi-automatically generated dataset tailored for evaluating temporal reasoning in LLMs.
          </p>
          <div class="pub-links">
            <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TEMPTABQA-C">data</a>
          </div>
        </div>
      </article>
      <!-- MMT-Bench -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/MMTabQA_poster_placeholder.png" alt="MMTABQA thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2106.12345" target="_blank" rel="noopener">MMT-Bench</a>
          </h3>
          
          <div class="pub-meta">
            <span class="authors">Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, Vivek Gupta, Mohit Bansal, Dan Roth</span>
            <span class="venue"> <a href="https://2024.emnlp.org/" >EMNLP 2024</a></span>
          </div>
          <p class="pub-abstract">
           MMTabQA is a large-scale multimodal table QA dataset containing 69,740 questions over 25,026 tables, created by augmenting four existing datasets—WikiSQL (21,472 Qs, 9,784 tables), WikiTableQuestions (10,052 Qs, 1,259 tables), FeTaQA (7,476 Qs, 5,898 tables), and HybridQA (30,470 Qs, 8,085 tables).
It integrates multiple reasoning styles: SQL-based parsing, complex multi-row/column reasoning, long-form answers, and hybrid table–text reasoning with contextual passages.
Questions are categorized into 4 types—Explicit (24,797), Implicit (21,453), Visual (5,763), and Answer-Mention (17,727)—with an average of 14.10 images per table, enabling evaluation of entity parsing, visual grounding, and multimodal reasoning skills.
          </p>
          <div class="pub-links">
            <a class="btn btn-code" href="https://github.com/OpenGVLab/MMT-Bench">data</a>
          </div>
        </div>
      </article>

      
      <!-- FlowVQA -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/flowqa_poster_placeholder.png" alt="FlowVQA thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2403.12345" target="_blank" rel="noopener">FlowVQA: Visual Question Answering over Flowcharts</a>
          </h3>
          <div class="pub-meta">
            <span class="authors">Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth</span>
            <span class="venue"> <a href="https://2024.aclweb.org/" >ACL 2024</a></span>
          </div>
          <p class="pub-abstract">
            FlowVQA is a visual question answering benchmark built around 2,272 human-verified flowchart images and 22,413 Q&A pairs, sourced from WikiHow (1,121), Instructables (701), and FloCo (450).
Questions span four categories—Fact Retrieval, Applied Scenario, Flow Referential, and Topological—to assess multimodal LLM skills in visual grounding, logical progression, and spatial reasoning.
Baseline evaluations show even the best-performing approach (GPT-4 with directive-based few-shot prompting) reaches only 68.42% majority voting accuracy, highlighting the challenge of structured visual logic understanding.
          </p>
          <div class="pub-links">
            <a class="btn btn-code" href="https://github.com/flowvqa/flowvqa">data</a>
          </div>
        </div>
      </article>


      <!-- InfoSync -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/INFOSYNC.png" alt="InfoSync thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2401.12345" target="_blank" rel="noopener">InfoSync: Synchronizing Information Across Tables</a>
          </h3>
          
          <div class="pub-meta">
            <span class="authors">Sidharth Khincha, Chelsi Jain, Vivek Gupta, Tushar Kataria, Shuo Zhang</span>
            <span class="venue"> <a href="https://2023.aclweb.org/" >ACL 2023</a></span>
          </div>
          <p class="pub-abstract">
            InfoSync is a large-scale multilingual table synchronization dataset containing ~99,440 Wikipedia infoboxes (1,078,717 rows) across 14 languages, spanning 22 diverse categories such as Airport, Album, City, Company, Country, and Person.
It includes both translation-based and native-annotated test sets (~3,500 table pairs) for evaluating two core tasks: Information Alignment (row mapping across languages) and Information Update (propagating missing or outdated rows).
Human-assisted Wikipedia edits using InfoSync updates achieved a 77.28% acceptance rate, demonstrating its utility for improving cross-lingual consistency in semi-structured data.
          </p>
          <div class="pub-links">
            <a class="btn btn-code" href="https://github.com/Info-Sync/InfoSync">data</a>
          </div>
        </div>
      </article>


      <!-- InfoTab -->
      <article class="pub-item">
        <figure class="thumb">
          <img src="/poster_docs/Infotabs_poster_placeholder.png" alt="InfoTab thumbnail">
        </figure>
        <div class="pub-body">
          <h3 class="pub-title">
            <a href="https://arxiv.org/abs/2104.08320" target="_blank" rel="noopener">InfoTab: A Benchmark for Table-based Information Extraction</a>
          </h3>
           <div class="pub-meta">
            <span class="authors">Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar</span>
            <span class="venue"><a href="https://acl2020.org/" >ACL 2020</a></span>
          </div>
          <p class="pub-abstract">

          <p class="pub-abstract">
            INFOTABS is a dataset of 23,738 premise–hypothesis pairs, where all premises are info-boxes and the hypotheses are short sentences.
It contains 2,540 unique info-boxes from Wikipedia articles across various categories, with hypotheses written by Amazon Mechanical Turk workers.
Determining the correct label—entailment, contradiction, or unrelated—often requires multiple inferences across table rows combined with world knowledge, and verification confirms its high quality.
          </p>
          <div class="pub-links">
            <a class="btn btn-paper" href="https://github.com/infotabs/infotabs">data</a>
          </div>
        </div>
      </article>
  </section>
</section>
<!-- Data Section -->
<section id="demo" style="margin-top: 4rem;">
  <h2>Demo</h2>
  <section class="selected-publications">
    <h2 class="section-title">Software</h2>
    <div class="pub-list">

      <!--Prasie-->
      <article class="pub-item">
      <figure class="thumb">
        <img src="/poster_docs/Praise_poster_placeholder.png" alt="paper thumbnail" />
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-demo.62.pdf" target="_blank" rel="noopener">PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</a>
        </h3>
  <div class="pub-meta">
            <span class="authors">Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta</span>
            <span class="venue"> <a href="https://2025.aclweb.org/" >ACL 2025</a></span>
          </div>
        <p class="pub-abstract">
          Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://praise-demo.vercel.app/">demo</a>

        </div>
      </div>
    </article>
</section>

<style>
/* Basic styling — tweak to match fonts/colors of your site */
.selected-publications {
  max-width: 980px;
  margin: 2rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;
}

  .past-publications {
  max-width: 980px;
  margin: 2rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;

}
.section-title {
  font-size: 1.5rem;
  font-weight: 700;
  color: #8C1D40; /* ASU Maroon */
  text-decoration: none;
  margin-bottom: 1rem;
  padding-top: 0.75rem;
}  
}
.subsection-title {
  font-size: 1rem;
  font-weight: 700;
  margin-bottom: 1rem;
  padding-top: 0.75rem;
}

.pub-list {
  display: grid;
  gap: 1.25rem;
}

.pub-item {
  display: grid;
  grid-template-columns: 120px 1fr;
  gap: 1rem;
  align-items: start;
  padding: 0.75rem 0;
  border-bottom: 1px solid rgba(0,0,0,0.04);
}

.thumb {
  margin: 0;
  width: 120px;
  height: auto;
}
.thumb img {
  width: 100%;
  height: 100%;
  border-radius: 4px;
  object-fit: cover;
  border: 1px solid #eee;
}

.pub-title {
  margin: 0 0 0.35rem 0;
  font-size: 1.05rem;
}
.pub-title a {
  color: #8C1D40; /* ASU Maroon */
  text-decoration: none;
  font-weight: 700;
}
.pub-title a:hover { text-decoration: underline; }

.pub-meta {
  font-size: 0.9rem;
  color: #222;
  margin-bottom: 0.4rem;
}
.pub-meta .authors { display: block; color: ##000000; font-weight: 500; }
.pub-meta .venue { display: block; color: inherit; font-weight: 700; margin-top: 0.15rem } 

.pub-abstract {
  margin: 0.5rem 0 0.6rem 0;
  color: #555;
  line-height: 1.45;
  text-align: justify
}

.pub-links {
  display: flex;
  gap: 0.5rem;
}
.btn {
  display: inline-block;
  padding: 0.32rem 0.6rem;
  border-radius: 6px;
  font-size: 0.85rem;
  text-decoration: none;
  border: 1px solid transparent;
  box-sizing: border-box;
}
.btn-paper {
  background: #FFC627;
  color: #000000;
  border-color: rgba(11,99,214,0.12);
}
.btn-site {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}

.btn-code {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}
.btn-slides {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}
/* Responsive fallback */
@media (max-width: 740px) {
  .pub-item { grid-template-columns: 1fr; }
  .thumb { width: 100%; }
}
</style>
</section></div></section>


    <script id="dsq-count-scr" src="//kordinglab.disqus.com/count.js" async></script>
    <script src="http://code.jquery.com/jquery-2.2.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    var pageTracker = _gat._getTracker("UA-34145995-1");
    pageTracker._trackPageview();
    </script>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-34145995-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-34145995-1');
    </script>

</body>
</html>
