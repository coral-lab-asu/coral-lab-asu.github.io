---
layout: default
title: Publications
permalink: /publication/
---

<!-- Google Scholar Link -->
<div class="google-scholar-link">
  <a href="https://scholar.google.com/citations?user=Bs5H0S4AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">
    View All Publications on Google Scholar
  </a>
</div>


<!-- selected-publications.html -->
<section class="selected-publications">
  <h2 class="section-title">RECENT PUBLICATIONS</h2>
  <h3 class="subsection-title">Complex Data (Structured Data)</h3>

  <div class="pub-list">

    <!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MapIQ_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2507.11625" target="_blank" rel="noopener">MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
            V Srivastava, F Lei, S Mukhopadhyay, V Gupta, R Maciejewski
          </span>
          <span class="venue">
             <a href="https://colmweb.org/" target="_blank" rel="noopener">COLM 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          Recent advancements in multimodal large language models (MLLMs) have
 driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to
 visual question answering with maps (Map-VQA). However, Map-VQA
 research has primarily focused on choropleth maps, which cover only a
 limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising
 14,706 question-answer pairs across three map types—choropleth maps,
 cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another
 and a human baseline. An additional experiment...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2507.11625" target="_blank" rel="noopener">paper</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/TabXEval_Poster.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://coral-lab-asu.github.io/tabxeval/" target="_blank" rel="noopener">TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
          Vihang Pancholi, Jainit Sushil Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta
          <span class="venue"> 
          <a href="https://2025.aclweb.org/">ACL 2025</a>
          </span>
         
        </div>

        <p class="pub-abstract">
 Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard
 metrics often overlook subtle structural and
 content-level discrepancies. To address this,
 we propose a rubric-based evaluation frame
work that integrates multi-level structural descriptors with fine-grained contextual signals,
 enabling more precise and consistent table
 comparison. Building on this, we introduce
 TabXEval, an eXhaustive and eXplainable
 two-phase evaluation framework. TabXEval first aligns reference and predicted...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-acl.1176.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/tabxeval/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TabXEval">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/TabXEval_presentation.pdf" target="_blank">slides</a>

        </div>
      </div>
    </article>

    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/M&M_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-long.1460.pdf" target="_blank" rel="noopener">Map&Make: Schema Guided Text to Table Generation</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Naman Ahuja, Fenil Bardoliya, Chitta Baral, Vivek Gupta</span>
          <span class="venue">
          <a href="https://2025.aclweb.org/">ACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          Transforming dense, unstructured text into interpretable tables—commonly referred to as Text-to-Table generation—is a key task in information extraction. Existing methods often overlook what complex information to extract and how to infer it from text. We present Map&Make, a versatile approach that decomposes text into atomic propositions to infer latent schemas, which are then used to generate tables capturing both qualitative nuances and quantitative facts. We evaluate...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-long.1460.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/map-make/">website</a>
          <a class="btn btn-code" href="https://github.com/coral-lab-asu/map-make/">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/Map&Make_presentation.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Praise_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-demo.62.pdf" target="_blank" rel="noopener">PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-demo.62.pdf">paper</a>
          <a class="btn btn-site" href="https://project-praise.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/project-PRAISE/system-demo/">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/LLM_Symbolic_Poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-acl.1022.pdf" target="_blank" rel="noopener">LLM-Symbolic Integration for Robust Temporal Tabular Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Atharv Kulkarni, Kushagra Dixit, Vivek Srikumar, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Temporal tabular question answering presents a significant challenge for Large Language Models (LLMs), requiring robust reasoning over structured data—a task where traditional prompting methods often fall short. These methods face challenges such as memorization, sensitivity to table size, and reduced performance on complex queries. To overcome these limitations, we introduce TEMPTABQA-C, a synthetic dataset designed for systematic and controlled evaluations, alongside a symbolic intermediate representation that transforms tables into database schemas. This structured approach allows LLMs to generate and execute SQL queries, enhancing generalization and mitigating biases. By incorporating...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-acl.1022.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/llm_symbolic/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TEMPTABQA-C">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/LLM-Symbolic_presentation.pdf" target="_blank">slides</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Getreason_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-long.1439.pdf" target="_blank" rel="noopener">GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Publicly significant images from events carry
 valuable contextual information with applications in domains such as journalism and education. However, existing methodologies of ten struggle to accurately extract this contextual relevance from images. To address
 this challenge, we introduce GETREASON(Geospatial Event Temporal Reasoning), a
 framework designed to go beyond surface
level image descriptions and infer deeper contextual meaning. We hypothesize that extracting global event, temporal, and geospatial information from an image enables a
enables a more accurate understanding of its contextual significance. We also introduce a new
 metric GREAT (Geospatial, Reasoning and
 Event Accuracy with Temporal alignment) for
 a reasoning capturing evaluation. Our layered multi-agentic approach, evaluated...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-long.1439.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-asu.github.io/getreason/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/getreason">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/Getreason_presentation.pdf" target="_blank">slides</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MapWise_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/abs/2409.00255" target="_blank" rel="noopener">MAPWise: Evaluating Vision-Language Models for Advanced Map Queries</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Manish Shrivastava, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
            
             Vision-language models (VLMs) excel at tasks
 requiring joint understanding of visual and linguistic information. A particularly promising
 yet under-explored application for these models
 lies in answering questions based on various
 kinds of maps. This study investigates the
 efficacy of VLMs in answering questions based
 on choropleth maps, which are widely used for
 data analysis and representation. To facilitate
 and encourage research in this area, we introduce a novel map-based question-answering
 benchmark, consisting of maps from three
 geographical regions (United States, India,
 China), each containing 1000 questions. Our
 benchmark...
            
            
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/abs/2409.00255">paper</a>
          <a class="btn btn-site" href="https://map-wise.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/abhishekrajgaria/Mapwise">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Infotabs_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://example.com/paper2" target="_blank" rel="noopener">Leveraging LLM for Synchronizing Information Across Multilingual Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.329/">paper</a>
          <a class="btn btn-site" href="https://zero-shot-llm-infosync.github.io/zero-shot-llm-infosync/">website</a>
          

          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/TRANSIENTTABLES_Poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.332.pdf" target="_blank" rel="noopener">TRANSIENT TABLES: Evaluating LLMs’ Reasoning on Temporally Evolving Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Humans continuously make new discoveries,
 and understanding temporal sequence of events
 leading to these breakthroughs is essential for
 advancing science and society. This ability to
 reason over time allows us to identify future
 steps and understand the effects of financial
 and political decisions on our lives. However,
 large language models (LLMs) are typically
 trained on static datasets, limiting their ability
 to perform effective temporal reasoning. To
 assess the temporal reasoning capabilities of
 LLMs, we present the TRANSIENTTABLES
 dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238
 entities across multiple time periods. We introduce a template-based question-generation
 pipeline that harnesses LLMs to refine both
 templates and questions. Additionally, we establish baseline...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.332.pdf">paper</a>
          <a class="btn btn-site" href="https://transienttables.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/harsh1399/TransientTables">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/TRANSIENTTABLES_presentation.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/H-Star_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.445.pdf" target="_blank" rel="noopener">H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Nikhil Abhyankar, Vivek Gupta, Dan Roth, Chandan Reddy</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Tabular reasoning involves interpreting natural language queries about tabular data, which presents a unique challenge of combining language understanding with structured data analysis. Existing methods employ either textual reasoning, which excels in semantic interpretation but struggles with mathematical
 operations, or symbolic reasoning, which handles computations well but lacks semantic understanding. This paper introduces a novel
 algorithm H-STAR that integrates both symbolic and semantic (textual) approaches in a
 two-stage process to address these limitations.
 H-STAR employs: (1) step-wise table extraction using ‘multi-view’ column retrieval followed by row extraction, and (2) adaptive reasoning that adapts reasoning strategies based on question types, utilizing semantic reasoning for direct lookup and complex lexical queries while augmenting textual reasoning with symbolic reasoning support for quantitative and logical tasks. Our extensive...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.445.pdf">paper</a>
          <a class="btn btn-site" href="https://hstar-llm.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/nikhilsab/H-STAR">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Enhancing_temporal_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-naacl.278.pdf" target="_blank" rel="noopener">Enhancing Temporal Understanding in LLMs for Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Irwin Deng, Kushagra Dixit, Dan Roth, Vivek Gupta</span>
          
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a> (Findings)
              </span>
        </div>

        <p class="pub-abstract">
           Temporal reasoning over tabular data presents
 substantial challenges for large language models (LLMs), as evidenced by recent research. In
 this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific
 limitations of LLMs. Our investigation leads
 to enhancements in TempTabQA, a benchmark
 specifically designed for tabular temporal question answering. We provide critical insights
 for enhancing LLM performance in temporal
 reasoning tasks with tabular data. Furthermore,
 we introduce a novel approach, C.L.E.A.R to
 strengthen LLM capabilities in this domain.
 Our findings demonstrate that our method im
proves evidence-based reasoning across various
 models. Additionally, our experimental...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-naacl.278.pdf">paper</a>
          <a class="btn btn-site" href="https://clear-prompting.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/clear-temptabqa/clear_temptabqa">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/NTSEBench_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2407.10380" target="_blank" rel="noopener">NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Pranshu Pandya, Vatsal Gupta, Agney S Talwarr, Tushar Kataria, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
          
        </div>

        <p class="pub-abstract">
           Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand
 the ability to quickly reason, decipher, and
 evaluate patterns both textually and spatially.
 Due to extensive training on vast amounts of
 human-curated data, large language models
 (LLMs) and vision language models (VLMs)
 excel in common-sense reasoning tasks, but
 still struggle with more complex reasoning
 that demands deeper cognitive understanding.
 We introduce NTSEBENCH, a new dataset designed to evaluate cognitive multimodal reasoning and problem-solving skills of large models. The dataset contains 2,728 multiple-choice
 questions, accompanied by a total of 4,642 images, spanning 26 categories....
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2407.10380">paper</a>
          <a class="btn btn-site" href="https://ntsebench.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/NTSEBench/NTSEBench">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Infotabs_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.emnlp-main.1237.pdf" target="_blank" rel="noopener">Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Vatsal Gupta, Pranshu Pandya, Tushar Kataria, <b>Vivek Gupta</b>, Dan Roth <br></span>
          <span class="venue">
              <a href="https://2024.emnlp.org/">EMNLP 2024</a>
             </span>
        </div>

        <p class="pub-abstract">
          Language models, characterized by their black
box nature, often hallucinate and display sensitivity to input perturbations, causing concerns
 about trust. To enhance trust, it is imperative
 to gain a comprehensive understanding of the
 model’s failure modes and develop effective
 strategies to improve their performance. In this
 study, we introduce a methodology designed
 to examine how input perturbations affect language models across various scales, including
 pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance
 the model’s robustness to input perturbations.
 Additionally, ...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.emnlp-main.1237.pdf">paper</a>
          <a class="btn btn-site" href="https://msin-infotabs.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/msin-infotabs/Evaluating-Concurrent-Robustness-of-Language-Models-Across-Diverse-Challenge-Sets">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Unravelling_truth_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.973.pdf" target="_blank" rel="noopener">Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"><br>
  Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, <b>Vivek Gupta</b>, Dan Roth <br></span>
          
          <span class="venue">
          <a href="https://2024.emnlp.org/">EMNLP 2024</a> (Findings)
          </span>
        </div>

        <p class="pub-abstract">
          Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally,...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.973/">paper</a>
          <a class="btn btn-site" href="https://robustcqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/RobustCQA/Robust-CQA">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MMTabQA_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.822.pdf" target="_blank" rel="noopener">Knowledge-Aware Reasoning over Multimodal Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, <b>Vivek Gupta</b>, Mohit Bansal, Dan Roth</span>
          <span class="venue">
              <a href="https://2024.emnlp.org/">EMNLP 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
           Existing datasets for tabular question answering typically focus exclusively on text within
 cells. However, real-world data is inherently
 multimodal, often blending images such as
 symbols, faces, icons, patterns, and charts
 with textual content in tables. With the evolution of AI models capable of multimodal
 reasoning, it is pertinent to assess their efficacy in handling such structured data. This
 study investigates whether current AI models
 can perform knowledge-aware reasoning on
 multimodal structured data. We explore their
 ability to reason on tables that integrate both
 images and text, introducing MMTABQA, a
 new dataset designed for this purpose. Our
 experiments...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.822.pdf">paper</a>
          <a class="btn btn-site" href="https://mmtabqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/MMTabQA/mmtabqa">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/mmtabqa_slides.pdf" target="_blank">slides</a>
        </div>
      </div>
    </article>


<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/flowqa_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.78.pdf" target="_blank" rel="noopener">FlowVQA: Mapping Multimodal Logic in
 Visual Question Answering with Flowcharts</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, <b>Vivek Gupta</b>, Dan Roth</span>
          <span class="venue">
              <a href="https://2024.aclweb.org/">ACL 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
            Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual
 contexts. FlowVQA comprises 2,272 carefully
 generated and human-verified flowchart images
 from three distinct content sources, along with
 22,413 diverse question-answer pairs, to test a
 spectrum of reasoning tasks, including information localization, decision-making, and logical
 progression. We conduct a thorough baseline
 evaluation on a suite of both open-source and
 proprietary multimodal language models using
 various strategies, followed by an analysis of
 directional bias. The results...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.78.pdf">paper</a>
          <a class="btn btn-site" href="https://flowvqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/flowvqa">code</a>
        </div>
      </div>
      </article>


<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Evaluating_LM_Math_Reasoning_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.231.pdf" target="_blank" rel="noopener">Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Pragya Srivastava, Manuj Malik, <b>Vivek Gupta</b>, Tanuja Ganu, Dan Roth</span>
          <span class="venue">
              <a href="https://2024.aclweb.org/">ACL 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
            Large Language Models (LLMs), excel in
 natural language understanding, but their capability for complex mathematical reasoning
 with a hybrid of structured tables and unstructured text remain uncertain. This study explores LLMs’ mathematical reasoning on four
 financial tabular question-answering datasets:
 TATQA, FinQA, ConvFinQA, and Multihiertt.
 Through extensive experiments with various
 models and prompting techniques, we assess
 how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to
 table complexity and performance variations
 with an increasing number of arithmetic reasoning steps. The results...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.231.pdf">paper</a>
          
        </div>
      </div>
    </article>
    
<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/ChartCheck_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.828.pdf" target="_blank" rel="noopener">ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World Chart Images</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Mubashara Akhtar, Nikesh Subedi, <b>Vivek Gupta</b>, Sahar Tahmasebi, Oana Cocarascu, Elena Simperl</span>
          <span class="venue">
              <a href="https://2024.aclweb.org/">ACL 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
           Whilst fact verification has attracted substantial interest in the natural language processing
 community, verifying misinforming statements
 against data visualizations such as charts has
 so far been overlooked. Charts are commonly
 used in the real-world to summarize and communicate key information, but they can also
 be easily misused to spread misinformation
 and promote certain agendas. In this paper,
 we introduce ChartCheck, a novel, large-scale
 dataset for explainable fact-checking against
 Chart:
 Evidence
 real-world charts, consisting of 1.7k charts and
 10.5k human-written claims and explanations.
 We systematically...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.828.pdf">paper</a>
          <a class="btn btn-code" href="https://github.com/mubasharaak/ChartCheck">code</a>
        </div>
      </div>
    </article>
    
<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Enhancing_Question_Answering_on_Charts_QA_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.822.pdf" target="_blank" rel="noopener">Enhancing Question Answering on Charts Through Effective Pre-training Tasks</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Ashim Gupta, <b>Vivek Gupta</b>, Shuo Zhang, Yujie He, Ning Zhang, Shalin Shah </span>
          <span class="venue">
              <a href="https://blackboxnlp.github.io/">BlackboxNLP 2024</a> (Findings)
              
           </span>  
        </div>

        <p class="pub-abstract">
            To completely understand a document, the use
 of textual information is not enough. Understanding visual cues, such as layouts and charts,
 is also required. While the current state-of
the-art approaches for document understanding
 (both OCR-based and OCR-free) work well,
 we have not found any other works conducting a thorough analysis of their capabilities and limitations. Therefore, in this work, we address the limitation of current VisualQA models when applied to charts and plots. 
To investigate shortcomings of the state-of-the-art models, we conduct a comprehensive behavioral analysis, using ChartQA as a case study. Our findings ...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.blackboxnlp-1.11.pdf">paper</a>
          
        </div>
      </div>
    </article>
<hr>

<section class="past-publications">

     <h2 class="section-title">PAST PUBLICATIONS</h2>
     <h3 class="subsection-title">Semi-structured Data (Tabular Reasoning)</h3>
<ul>


  
  <li><b>TempTabQA: Temporal Question Answering for Semi-Structured Tables</b><br>
  <b>Vivek Gupta</b>, Pranshu Kandoi, Mahek Bhavesh Vora, Shuo Zhang, Yujie He, Ridho Reinanda, Vivek Srikumar <br>
  Published at <a href="https://2023.emnlp.org/">EMNLP 2023</a>, <a href="https://aclanthology.org/2023.emnlp-main.149.pdf">Paper</a>, 
    <a href="https://temptabqa.github.io/">Project Page</a>
  </li>

  <li><b>Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data</b><br>
  Mubashara Akhtar, Abhilash Shankarampeta, <b>Vivek Gupta</b>, Arpit Patil, Oana Cocarascu, Elena Simperl <br>
  Published at <a href="https://2023.emnlp.org/">EMNLP 2023(Findings)</a>, <a href="https://aclanthology.org/2023.findings-emnlp.1028.pdf">Paper</a>
  </li>

  <li><b>InfoSync: Information Synchronization across Multilingual Semi-structured Tables</b><br>
  Sidharth Khincha, Chelsi Jain, <b>Vivek Gupta</b>, Tushar Kataria, Shuo Zhang <br>
  Published at <a href="https://2023.aclweb.org/">ACL 2023</a>, presented at <a href="https://megagon.ai/matching-2023/">Matching@ACL 2023</a> <a href="https://info-sync.github.io/info-sync/">Project Page</a>, <a href="https://aclanthology.org/2023.findings-acl.159/">Paper</a>, <a href="https://drive.google.com/file/d/18b67cU2XDxMZpJ-99VdWeL8F3lBGyaLj/view">Video</a>, <a href="https://vgupta123.github.io/docs/infosync_poster.pdf">Poster</a>, <a href="https://docs.google.com/presentation/d/1lPm7c8hubwADNpWHfcqNCRX-KlChh8gEzWPBLIFP79Y/edit?usp=sharing">PPT</a>
  </li>

  <li><b>Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning</b>, <br/> 
    <b>Vivek Gupta</b>, Shuo Zhang, Alakananda Vempala, Yujie He, Temma Choji, Vivek Srikumar <br/> published at 
    <a href="https://www.2022.aclweb.org/">ACL 2022</a> <a href="https://aclanthology.org/2022.acl-long.231.pdf">Paper</a>
    <a href="docs/ACL-2022_Poster.pdf">Poster</a> [<a href="docs/ACL-2022-PPT.pptx">PPT</a>]
    <a href="https://youtu.be/YwGik-QCntA">Video</a>
    <a href="https://www.bloomberg.com/company/stories/bloomberg-ai-group-cto-office-publish-8-research-papers-at-acl-2022-dublin/">Media</a>
    <a href="https://www.linkedin.com/posts/shawn-edwards-0902a28b_bloombergs-ai-group-cto-office-publish-activity-6935303372510027776-SbHp?utm_source=share&amp;utm_medium=member_desktop">LinkedIn</a>
</li>

<li><b>Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning</b>, <br /> <b>Vivek Gupta</b>, Riyaz A. Bhat, Atreya Ghosal, Manish Srivastava, Maneesh Singh, Vivek Srikumar <br /> published at <a href="https://transacl.org/index.php/tacl">TACL 2022</a>, presented at <a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="https://aclanthology.org/2022.tacl-1.38.pdf">Paper</a>][<a href="https://arxiv.org/pdf/2108.00578.pdf">Preprint</a>] [<a href="docs/TACL-2022_Poster-Final.pdf">Poster</a>] [<a href="docs/TACL-2022_PPT.pdf">PPT</a>] [<a href="https://youtu.be/VRvttI4Ppso">Video</a>]
</li>

<li><b>Bilingual Tabular Inference: A Case Study on Indic Languages</b> <br /> Chaitanya Agarwal*, <b>Vivek Gupta</b>*, Anoop Kunchukuttan, Manish Shrivastava <br /> published at <a href="https://2022.naacl.org">NAACL 2022</a> [<a href="https://aclanthology.org/2022.naacl-main.295.pdf">Paper</a>] [<a href="docs/NAACL2022.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1OEhEcHhr8NlSAutqJhYQc4D_0oW0M5AIAGql5hoE31I/edit?usp=sharing">PPT</a>] [<a href="https://docs.google.com/presentation/d/1kHmZ1r2a8iA7tdCTS4ISDbEGFeAFJwyPTX1tOkCTi3M/edit?usp=sharing">Poster</a>] [<a href="https://www.youtube.com/watch?v=NXGUYl_en98&amp;ab_channel=ChaitanyaAgarwal">Video</a>]
</li>

<li><b>Trans-KBLSTM: An External Knowledge Enhanced Transformer BiLSTM model for Tabular Reasoning</b>, <br /> Yerram Varun*, Aayush Sharma*, <b>Vivek Gupta</b>* <br /> to appear at <a href="https://sites.google.com/view/deelio-ws">DeeLIO-2022</a> @<a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="https://aclanthology.org/2022.deelio-1.7.pdf">Paper</a>] [<a href="docs/TransKBLSTM.pdf">Preprint</a>] [<a href="docs/Trans-KBLSTM_Poster.pdf">Poster</a>] [<a href="docs/Trans-KBLSTM_PPT.pdf">PPT</a>] [<a href="https://youtu.be/CldBchM8IK0">Video</a>] <br />
<tt>Won Best Paper award at <a href="https://sites.google.com/view/deelio-ws">DeeLIO-2022</a></tt>
</li>

<li><b>XInfoTabS: Evaluating Multilingual Tabular Natural Language Inference</b>, <br />  Bhavnick Minhas*, Anant Shankhdhar*, <b>Vivek Gupta</b>*, Divyanshu Aggarwal, Shuo Zhang,<br /> published at <a href="https://mml-workshop.github.io">MML-2022</a> (non-archival) and <a href="https://fever.ai">FEVER-2022</a> (archival) @<a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="docs/XInfoTabS.pdf">Preprint</a>] [<a href="docs/XInfoTabS_Poster.pdf">Poster</a>] [<a href="docs/XInfoTabS_PPT.pdf">PPT</a>] [<a href="https://youtu.be/A9aS7sXY3BA">Video</a>] [<a href="https://www.bloomberg.com/company/stories/bloomberg-ai-group-cto-office-publish-8-research-papers-at-acl-2022-dublin/">Media</a>] [<a href="https://www.linkedin.com/posts/shawn-edwards-0902a28b_bloombergs-ai-group-cto-office-publish-activity-6935303372510027776-SbHp?utm_source=share&amp;utm_medium=member_desktop">LinkedIn</a>]
</li>

<li><b>Enhancing Tabular Reasoning with Pattern Exploiting Training</b>,<br /> Abhilash Shankarampeta*,  <b>Vivek Gupta</b>*, Shuo Zhang <br /> to appear at <a href="https://suki-workshop.github.io/">SUKI-2022</a> (non-archival) [<a href="https://suki-workshop.github.io/assets/paper/24.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1XmfE0YGlQ3wBTtHfAYi2edjaqk6j-LXTHNq2YS1lbtY/edit?usp=sharing">PPT</a>] [<a href="https://drive.google.com/file/d/12ex4MOIzR6soRZDLcRLkp8sFy0G0XlOg/view?usp=sharing">Poster</a>] [<a href="https://youtu.be/fHwaoawMg5s">Video</a>]<br /> (Extended Version at <a href="https://www.aacl2022.org/">AACL 2022</a>) [<a href="https://vgupta123.github.io/docs/pettable.pdf">Paper</a>] [<a href="https://infoadapet.github.io/">Project Page</a>] [<a href="https://www.bloomberg.com/company/stories/bloombergs-ai-engineering-group-publishes-4-nlp-research-papers-at-aacl-ijcnlp-2022/">Media</a>]
</li>

<li><b>Efficient Realistic Data Generation Framework for Semi-Structured Tabular Inference</b>, <br />
Dibyakanti Kumar*, <b>Vivek Gupta</b>*, Soumya Sharma, Shuo Zhang <br /> to appear at <a href="https://suki-workshop.github.io/">SUKI-2022</a>(non-archival)  [<a href="https://suki-workshop.github.io/assets/paper/25.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1WfB3Rx4m81d4T8u9QVTsKeonCoOZrzwG6RN0JyXi9pU/edit?usp=sharing">PPT</a>] [<a href="https://www.youtube.com/watch?v=6gDzTIYH6Rw&amp;feature=youtu.be&amp;ab_channel=DibyakantiKumar">Video</a>] [<a href="https://drive.google.com/file/d/1sTvTcLgoqyJP2cAXsSM0t7j9sfUWI9oO/view?usp=sharing">Poster</a>]<br /> (Extended Version at <a href="https://2022.emnlp.org/">EMNLP 2022</a>) [<a href="https://autotnli.github.io/">Project Page</a>] [<a href="https://vgupta123.github.io/docs/autotnli.pdf">Paper</a>] [<a href="https://www.bloomberg.com/company/stories/bloombergs-ai-engineering-group-cto-publish-5-nlp-research-papers-at-emnlp-2022/?linkId=192716295">Media</a>]
</li>

<li><b>Leveraging Data Recasting to Enhance Tabular Reasoning</b>,<br />
Aashna Jena*,  <b>Vivek Gupta</b>*, Manish Shrivastava, Julian Martin Eisenschlos <br /> to appear at <a href="https://suki-workshop.github.io/">SUKI-2022</a> (non-archival) [<a href="https://suki-workshop.github.io/assets/paper/26.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1nOA-pmfP75GDhF8EgC9nagcy4j70otpLTHYpYCfRa2w/edit?usp=sharing">Poster</a>] [<a href="https://docs.google.com/presentation/d/1XtLoH7rt5phfSgR8kwadt9o-_7j2PUl2MOaKlgDgVb4/edit?usp=sharing">PPT</a>] [<a href="https://youtu.be/hZzvrYy3-3g">Video</a>] <br /> (Extended Version at <a href="https://2022.emnlp.org/">EMNLP 2022</a>) [<a href="https://recasting-to-nli.github.io/">Project Page</a>] [<a href="https://vgupta123.github.io/docs/recasting.pdf">Paper</a>] [<a href="https://ai.googleblog.com/2022/12/google-at-emnlp-2022.html">Media</a>] [<a href="https://docs.google.com/presentation/d/1ZClxfBJsKSc8a0NIojsOg-wWkxtw-WauI15bAT6ly5w/edit?usp=sharing">Poster</a>]
</li>

<li><b>RetroNLU: Retrieval Augmented Task Oriented Semantic Parsing</b>,<br />  <b>Vivek Gupta</b>, Akshat Shrivastava, Adithya Sagar, Armen Aghajanyan, Denis Savenkov,<br /> to appear at <a href="https://www.semiparametric.ml">Spa-NLP-2022</a> (non-archival) and <a href="https://sites.google.com/view/4thnlp4convai">NLP4ConvAI-2022</a> (archival) @<a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="https://aclanthology.org/2022.nlp4convai-1.15.pdf">Paper</a>] [<a href="docs/RetroNLU.pdf">Preprint</a>] [<a href="docs/RetroNLU_Poster.pdf">Poster</a>] [<a href="docs/RetroNLU_PPT.pdf">PPT</a>] [<a href="https://youtu.be/sAsaTTveCmE">Video</a>]<br />
<tt>Won Outstanding Paper award at <a href="https://sites.google.com/view/4thnlp4convai">NLP4ConvAI-2022</a></tt>
</li>

<li><b>TabPert: An Effective Platform for Tabular Perturbation</b>, <br /> Nupur Jain,<b>Vivek Gupta</b>, Anshul Rai, Gaurav Kumar <br /> Published at <a href="https://2021.emnlp.org/">EMNLP 2021</a>, Demo track [<a href="https://aclanthology.org/2021.emnlp-demo.39.pdf">Paper</a>] [<a href="https://tabpert.github.io">Project Page</a>][<a href="https://arxiv.org/pdf/2108.00603.pdf">Preprint</a>] [<a href="docs/tabpert_ppt.pdf">PPT</a>] [<a href="https://www.youtube.com/watch?v=zjfRk8--jEY&amp;ab_channel=VivekGupta">Video</a>] [<a href="https://github.com/utahnlp/tabpert">Code</a>] 
</li>

<li><b>Incorporating External Knowledge to Enhance Tabular Reasoning</b>, <br /> J. Neeraja*, <b>Vivek Gupta</b>*, and Vivek Srikumar <br /> Published at <a href="https://2021.naacl.org/">NAACL 2021</a> [<a href="https://www.aclweb.org/anthology/2021.naacl-main.224.pdf">Paper</a>] [<a href="https://knowledge-infotabs.github.io">Project Page</a>] [<a href="https://github.com/utahnlp/knowledge_infotabs">Code</a>] [<a href="https://screencast-o-matic.com/watch/crhlnpVfYcG">Video</a>] [<a href="docs/NAACL_2021_Poster.pdf">Poster</a>] [<a href="https://docs.google.com/presentation/d/1MfkHFcfnoCiBY9ftp8Uw_uosz_Xo5UQNkZEwJZ-HS10/edit?usp=sharing">PPT</a>]
</li>

<li><b>InfoTabS: Inference on Tables as Semi-structured Data</b>, <br /> <b>Vivek Gupta</b>, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar <br /> Published at <a href="https://acl2020.org">ACL 2020</a> [<a href="https://www.aclweb.org/anthology/2020.acl-main.210/">Paper</a>] [<a href="https://infotabs.github.io">Project Page</a>] [<a href="https://www.youtube.com/watch?v=YhfU1BON8EI">Video</a>] [<a href="https://github.com/infotabs/infotabs">Data</a>] [<a href="https://github.com/utahnlp/infotabs-code">Code</a>]
</li>

</ul>

<h3>Logic Consistency / Low-resource Applications</h3>
<ul>
  <li><b>IndicSemParse: Evaluating Inter-Bilingual Semantic Parsing for Indian Languages</b><br>
  Divyanshu Aggarwal*, <b>Vivek Gupta</b>*, Anoop Kunchukuttan <br>
  To appear at <a href="https://sites.google.com/view/5thnlp4convai/">NLP4ConvAI 2023</a> <a href="https://iesemparse.github.io/">Project Page</a>, <a href="https://arxiv.org/pdf/2304.13005.pdf">Preprint</a>
  </li>
  
  <li><b>IndicXNLI: Evaluating Multilingual Inference for Indian Languages</b><br>
  Divyanshu Aggarwal*, <b>Vivek Gupta</b>*, Anoop Kunchukuttan <br>
  To appear at <a href="https://mia-workshop.github.io/">MIA-2022</a> (non-archival) <a href="https://arxiv.org/pdf/2204.08776.pdf">Preprint</a>
  </li>

  <li><b>Logic Driven Classification for Low Resource Settings</b><br>
  Shagun Uppal, <b>Vivek Gupta</b>, Avinash Swaminathan, Debanjan Mahata, Rakesh Gosangi, Haimin Zhang, Rajiv Ratn Shah, Amanda Stent <br>
  Published at AACL-IJCNLP 2020 <a href="https://www.aclweb.org/anthology/2020.aacl-main.71.pdf">Paper</a>
  </li>

  <li><b>A Logic-Driven Framework for Consistency of Neural Models</b><br>
  Tao Li, <b>Vivek Gupta</b>, Maitrey Mehta, and Vivek Srikumar <br>
  Published at EMNLP-IJCNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1405.pdf">Paper</a>
  </li>
</ul>

<h3>Fairness and Bias</h3>
<ul>
  <li><b>Unbiasing Review Ratings with Tendency-based Collaborative Filtering</b><br>
  Pranshi Yadav*, Priya Yadav*, Pegah Nokhiz, <b>Vivek Gupta</b><br>
  Published at <a href="https://aacl2020-srw.github.io">AACL-IJCNLP SRW 2020</a> <a href="https://www.aclweb.org/anthology/2020.aacl-srw.8.pdf">Paper</a>
  </li>

  <li><b>User Bias Removal in Review Score Prediction</b><br>
  Rahul Wadbude, <b>Vivek Gupta</b>, Dheeraj Mekala, Harish Karnick <br>
  Published at CoDS-COMAD 2018 and DAB@CIKM 2017 <a href="https://dl.acm.org/citation.cfm?id=3152520">Paper</a>
  </li>

  <li><b>Equalizing Recourse across Groups</b><br>
  <b>Vivek Gupta</b>*, Pegah Nokhiz*, Chitradeep Dutta Roy*, Suresh Venkatasubramanian <br>
  Technical Report. <a href="https://arxiv.org/abs/1909.03166">Preprint</a>
  </li>

  <li><b>Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles</b> <br /> Dhruv Mahajan, <b>Vivek Gupta</b>, Satya Keerthi, Sundararjan Sellamanickam <br /> Technical Report. [<a href="https://arxiv.org/abs/1711.05482">Preprint</a>]
</li>

</ul>

<h3>Long-length Document Classification</h3>

<ul>
  
<li><b>Unsupervised Contextualized Document Representation</b>, <br /> Ankur Gupta, <b>Vivek Gupta</b> <br /> Published at <a href="https://sites.google.com/view/sustainlp2021/call-for-papers">SustaiNLP 2021</a> at <a href="https://2021.emnlp.org/">EMNLP 2021</a> workshop. [<a href="https://aclanthology.org/2021.sustainlp-1.17.pdf">Paper</a>] [<a href="https://arxiv.org/pdf/2109.10509.pdf">Preprint</a>] [<a href="docs/sustainlp21_ppt.pdf">PPT</a>] [<a href="docs/sustainlp21_poster.pdf">Poster</a>] [<a href="https://www.youtube.com/watch?v=xp576-9fZGw&amp;ab_channel=VivekGupta">Video</a>] [<a href="https://github.com/vgupta123/contextualize_scdv">Code</a>]
</li>
  
<li><b>Improving Document Classification with Multi-Sense Embeddings</b>, <br /> <b>Vivek Gupta</b>, Ankit Saw,  Pegah Nokhiz, Harshit Gupta, and Partha Talukdar <br /> Published at <a href="http://ecai2020.eu">ECAI 2020</a> [<a href="https://ecai2020.eu/papers/391_paper.pdf">Paper</a>] [<a href="http://vivgupt.blogspot.com/2019/06/document-vector-estimation-using.html">Blog</a>] [<a href="https://www.youtube.com/watch?v=tKY5P8JFw3Q">Video</a>] [<a href="https://github.com/vgupta123/SCDV-MS">Code</a>]  <br />(extention of <a href="https://naacl2019-srw.github.io/">NAACL-SRW 2019</a> work)<br />
</li>
  
<li><b>P-SIF: Document Embeddings using Partition Averaging</b> <br /> <b>Vivek Gupta</b>, Ankit Saw, Pegah Nokhiz,  Praneeth Netrapalli, Piyush Rai, Partha Talukdar<br /> Published at <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>, Presented at <a href="https://sites.google.com/view/sustainlp2020">SustaiNLP 2020</a> [<a href="docs/AAAI-GuptaV.3656.pdf">Paper</a>] [<a href="docs/aaai2020appendix.pdf">Appendix</a>] [<a href="docs/AAAI20_PPT.pdf">PPT</a>] [<a href="docs/AAAI20_Poster.pdf">Poster</a>] [<a href="https://github.com/vgupta123/P-SIF">Code</a>] [<a href="http://vivgupt.blogspot.com/2019/06/document-vector-estimation-using.html">Blog</a>]
</li>
  
<li><b>Word Polysemy Aware Document Vector Estimation</b> <br /> <b>Vivek Gupta</b>, Ankit Saw, Harshit Gupta, Pegah Nokhiz and Partha Talukdar <br /> Presented at <a href="https://naacl2019-srw.github.io/">NAACL-SRW 2019</a> (non-archival) <br />(extended version appear at <a href="http://ecai2020.eu">ECAI 2020</a>) [<a href="https://github.com/vgupta123/SCDV-MS">Code</a>]
</li>
  
<li><b>Sparse Composite Document Vectors using soft clustering over distributional representations</b> <br /> Dheeraj Mekala*, <b>Vivek Gupta</b>*, Bhargavi Paranjape , Harish Karnick <br /> Published at <a href="https://www.aclweb.org/anthology/events/emnlp-2017/">EMNLP 2017</a>. [<a href="http://aclweb.org/anthology/D17-1069">Paper</a>] [<a href="https://www.slideshare.net/vivekgupta3150/sparse-composite-document-vector-emnlp-2017">PPT</a>] [<a href="https://vimeo.com/238235553">Video</a>] [<a href="https://github.com/dheeraj7596/SCDV">Code</a>]
</li>
  
</ul>

<h3>Effective Dimentional Properties</h3>
<ul>
  
<li><b>On Dimensional Linguistic Properties of the Word Embedding Space</b> <br /> Vikas Raunak*, Vaibhav Kumar*, <b>Vivek Gupta</b> and Florian Metze <br /> Presented at <a href="https://sites.google.com/view/acl19studentresearchworkshop/">ACL-SRW 2019</a> (non-archival), Published at <a href="https://sites.google.com/view/repl4nlp2020/home">RepL4NLP 2020</a> [<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.19.pdf">Paper</a>] [<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.19/">Paper</a>] [<a href="https://github.com/vyraun/dlp">Code</a>] <br />
</li>
  
<li><b>Effective Dimensionality Reduction for Word Embeddings</b> <br /> Vikas Raunak, <b>Vivek Gupta</b> and Florian Metze <br /> Published at <a href="https://sites.google.com/view/repl4nlp2019/home">RepL4NLP 2019</a>. [<a href="https://www.aclweb.org/anthology/W19-4328">Paper</a>] [<a href="docs/RepL4NLP_Poster.pdf">Poster</a>] [<a href="https://github.com/vyraun/Half-Size">Code</a>]
</li>
  
</ul>

<h3>Summarization</h3>
<ul>
  <li><b>SumPubMed: Summarization Dataset of PubMed Scientific Articles</b><br>
  <b>Vivek Gupta</b>, Prerna Bharti, Pegah Nokhiz, Harish Karnick <br>
  Accepted to appear in ACL-IJCNLP SRW 2021 <a href="docs/121_paper.pdf">Preprint</a> <a href="https://github.com/vgupta123/sumpubmed">Dataset</a> <a href="https://docs.google.com/presentation/d/12zNpBw02DXjwQYTop5LQHwXe_HCLR8dg6D4bYYMYS7E/edit?usp=sharing">PPT</a>
  </li>

  <li><b>Unsupervised Semantic Abstractive Summarization</b><br>
  Shibhansh Dohare, <b>Vivek Gupta</b>, Harish Karnick <br>
  Published at ACL-SRW 2018 <a href="https://arxiv.org/pdf/1706.01678.pdf">Preprint</a> <a href="http://aclweb.org/anthology/P18-3011">Paper</a>
  </li>
</ul>

<h3>eXtreme Learning (Capturing Tail)</h3>
<ul>
  <li><b>Distributional Semantics meet Multi-Label Learning</b><br>
  <b>Vivek Gupta</b>, Rahul Wadbude, Nagararjan Natararjan, Harish Karnick, Prateek Jain, Piyush Rai <br>
  Published at AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4260">Paper</a>
  </li>

  <li><b>Bayes-optimal Hierarchical Classification over Asymmetric Tree-Distance Loss</b><br>
  Dheeraj Mekala, <b>Vivek Gupta</b>, Purushottam Kar, Harish Karnick <br>
  Technical Report. <a href="https://arxiv.org/pdf/1802.06771.pdf">Report</a>
  </li>

  <li><b>On Long-Tailed Phenomena in Neural Machine Translation</b>, <br /> Vikas Raunak, Siddharth Dalmia, <b>Vivek Gupta</b>, and Florian Metze <br /> Published at <a href="https://2020.emnlp.org">EMNLP 2020</a> (Findings), Presented at <a href="http://structuredprediction.github.io/SPNLP20">SPNLP2020</a> [<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.276.pdf">Paper</a>] [<a href="https://github.com/vyraun/long-tailed">Code</a>]
</li>

</ul>

<h3>Applications</h3>
<ul>
  <li><b>Product Classification in E-Commerce using Distributional Semantics</b><br>
  <b>Vivek Gupta</b>, Harish Karnick, Ashendra Bansal, Pradhuman Jhala <br>
  Published at COLING 2016 <a href="https://aclweb.org/anthology/C/C16/C16-1052.pdf">Paper</a>
  </li>

  <li><b>Assisting Humans to Achieve Optimal Sleep by Changing Ambient Temperature</b><br>
  <b>Vivek Gupta</b>*, Siddhant Mittal*, Sandip Bhaumik, Raj Roy <br>
  Published at BIBM 2016 <a href="http://ieeexplore.ieee.org/document/7822635/">Paper</a>
  </li>
</ul>


<style>
/* Basic styling — tweak to match fonts/colors of your site */
.selected-publications {
  max-width: 980px;
  margin: 2rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;
}

  .past-publications {
  max-width: 980px;
  margin: 2rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;

}
.section-title {
  font-size: 1.5rem;
  font-weight: 700;
  color: #8C1D40; /* ASU Maroon */
  text-decoration: none;
  margin-bottom: 1rem;
  padding-top: 0.75rem;
}  
}
.subsection-title {
  font-size: 1rem;
  font-weight: 700;
  margin-bottom: 1rem;
  padding-top: 0.75rem;
}

.pub-list {
  display: grid;
  gap: 1.25rem;
}

.pub-item {
  display: grid;
  grid-template-columns: 120px 1fr;
  gap: 1rem;
  align-items: start;
  padding: 0.75rem 0;
  border-bottom: 1px solid rgba(0,0,0,0.04);
}

.thumb {
  margin: 0;
  width: 120px;
  height: auto;
}
.thumb img {
  width: 100%;
  height: 100%;
  border-radius: 4px;
  object-fit: cover;
  border: 1px solid #eee;
}

.pub-title {
  margin: 0 0 0.35rem 0;
  font-size: 1.05rem;
}
.pub-title a {
  color: #8C1D40; /* ASU Maroon */
  text-decoration: none;
  font-weight: 700;
}
.pub-title a:hover { text-decoration: underline; }

.pub-meta {
  font-size: 0.9rem;
  color: #222;
  margin-bottom: 0.4rem;
}
.pub-meta .authors { display: block; color: ##000000; font-weight: 500; }
.pub-meta .venue { display: block; color: inherit; font-weight: 700; margin-top: 0.15rem } 

.pub-abstract {
  margin: 0.5rem 0 0.6rem 0;
  color: #555;
  line-height: 1.45;
  text-align: justify
}

.pub-links {
  display: flex;
  gap: 0.5rem;
}
.btn {
  display: inline-block;
  padding: 0.32rem 0.6rem;
  border-radius: 6px;
  font-size: 0.85rem;
  text-decoration: none;
  border: 1px solid transparent;
  box-sizing: border-box;
}
.btn-paper {
  background: #FFC627;
  color: #000000;
  border-color: rgba(11,99,214,0.12);
}
.btn-site {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}

.btn-code {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}
.btn-slides {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}

.google-scholar-link {
  text-align: center;
  margin: 0.5rem 0 0rem;
}

.google-scholar-link a {
  display: inline-block;
  background-color: #FFC627; /* ASU Gold */
  color: #000000;
  text-decoration: none;
  padding: 0.3rem 0.8rem;
  border-radius: 5px;
  font-weight: normal;
  transition: background-color 0.3s ease;
}

.google-scholar-link a:hover {
  background-color: #FFC627;
}

/* Responsive fallback */
@media (max-width: 740px) {
  .pub-item { grid-template-columns: 1fr; }
  .thumb { width: 100%; }
}
</style>
