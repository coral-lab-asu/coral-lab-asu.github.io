---
layout: default
title: Publications
permalink: /publication/
---

<!-- Google Scholar Link -->
<div class="google-scholar-link">
  <a href="https://scholar.google.com/citations?user=Bs5H0S4AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">
    View All Publications on Google Scholar
  </a>
</div>


<!-- selected-publications.html -->
<section class="selected-publications">
  <h2 class="section-title">RECENT PUBLICATIONS</h2>
  <h3 class="subsection-title">Complex Data (Structured Data)</h3>

  <div class="pub-list">

<!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MapVerse_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2602.10518" target="_blank" rel="noopener">MAPVERSE: A Benchmark for Geospatial Question Answering on Diverse
Real-World Maps
 </a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
           Sharat Bhat, Harshita Khandelwal, Tushar Kataria, Vivek Gupta

          </span>
          <span class="venue">
              <a href="https://wacv.thecvf.com/" target="_blank" rel="noopener">WACV 2026</a>
          </span>
        </div>

        <p class="pub-abstract">Maps are powerful carriers of structured and contextual
knowledge, encompassing geography, demographics, in-
frastructure, and environmental patterns. Reasoning over
such knowledge requires models to integrate spatial re-
lationships, visual cues, real-world context, and domain-
specific expertise-capabilities that current large language
models (LLMs) and vision–language models (VLMs) still
struggle to exhibit consistently. Yet, datasets used to bench-
mark VLMs on map-based reasoning remain narrow in
scope, restricted to specific domains, and heavily reliant
on artificially generated content (outputs from LLMs or
pipeline-based methods), offering limited depth for evaluat-
ing genuine geospatial reasoning. To address this gap, we
present MAPVERSE, a large-scale benchmark built on real-
world maps. It comprises...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2602.10518" target="_blank" rel="noopener">paper</a>
         
          
       
        </div>
      </div>
    </article>  
  


<!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/PerceptualObs_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2512.15949" target="_blank" rel="noopener">The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs

 </a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
           Tejas Anvekar, Fenil Bardoliya, Pavan Turaga, Chitta Baral, Vivek Gupta

          </span>
          <span class="venue">
              <a href="https://wacv.thecvf.com/" target="_blank" rel="noopener">WACV 2026</a>
          </span>
        </div>

        <p class="pub-abstract">We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words,systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2512.15949" target="_blank" rel="noopener">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/PerceptualObservatory/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/PerceptualObservatory">code</a>
          
       
        </div>
      </div>
    </article>  
  



<!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/sear_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2506.11246" target="_blank" rel="noopener">No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning

 </a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
           Abhishek Rajgaria,Kushagra Dixit, Mayank Vyas, Harshavardhan Kalalbandi, Dan Roth, Vivek Gupta

          </span>
          <span class="venue">
              <a href="https://2025.aaclnet.org/" target="_blank" rel="noopener">AACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
         Temporal Table Reasoning poses a significant challenge for Large Language Models (LLMs), requiring effective reasoning to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, model performance varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique on diverse table types to determine that performance depends on factors such as entity type, table structure, requirement of additional context and question complexity, with "NO" single method consistently outperforming others. To address this, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts to context and integrates structured reasoning. SEAR_Unified, its cost-efficient variant. We also demonstrate that optional table refactoring (preprocessing) enhances both approaches when tables lack structural consistency. Our results demonstrate that SEAR prompts achieve superior performance across all table types compared to baseline prompting techniques...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2506.11246" target="_blank" rel="noopener">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/SEAR/">website</a>
          <a class="btn btn-code" href="https://github.com/coral-lab-asu/SEAR/blob/main/dataset/">data</a>
          
       
        </div>
      </div>
    </article>  

   
<!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/mammqa_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2505.20816" target="_blank" rel="noopener">Rethinking Information Synthesis in Multimodal Question Answering
A Multi-Agent Perspective

 </a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
           Tejas Anvekar, Krishna Singh Rajput, Chitta Baral, Vivek Gupta

          </span>
          <span class="venue">
              <a href="https://2025.aaclnet.org/" target="_blank" rel="noopener">AACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
         Recent advances in multimodal question answering have primarily focused on combining
heterogeneous modalities or fine-tuning multimodal large language models. While these
approaches have shown strong performance,
they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting
both accuracy and interpretability. To address
these limitations, we propose MAMMQA , a
multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our
system includes...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2505.20816" target="_blank" rel="noopener">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/MAMMQA/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/MAMMQA">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/MAMMQA_Presentation.pdf" target="_blank">slides</a>
          
        
          
        </div>
      </div>
    </article>
    
<!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/sportsql_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2508.17157" target="_blank" rel="noopener">SPORTSQL: An Interactive System for Real-Time Sports Reasoning and
Visualization
 </a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
           Sebastian Martinez, Naman Ahuja, Fenil Bardoliya, Chris Bryan, Vivek Gupta
          </span>
          <span class="venue">
              <a href="https://2025.aaclnet.org/" target="_blank" rel="noopener">AACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          We present a modular, interactive system
SPORTSQL for natural language querying and
visualization of dynamic sports data, with a
focus on the English Premier League (EPL).
The system translates user questions into executable SQL over a live, temporally indexed
database constructed from real-time Fantasy
Premier League (FPL) data. It supports both
tabular and visual outputs, leveraging symbolic reasoning capabilities of Large Language
Models (LLMs) for query parsing, schema
linking, and visualization selection. To evaluate...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2508.17157" target="_blank" rel="noopener">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/SportsSQL/">website</a>
          <a class="btn btn-code" href="https://github.com/coral-lab-asu/SportSQL">code</a>
       
          
        </div>
      </div>
    </article>


<!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/interchart_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2508.07630" target="_blank" rel="noopener">InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
           Anirudh Iyengar Kaniyar Narayana Iyengar, Srija Mukhopadhyay, Adnan Qidwai, Shubhankar Singh, Dan Roth, Vivek Gupta
          </span>
          <span class="venue">
              <a href="https://2025.aaclnet.org/" target="_blank" rel="noopener">AACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2508.07630" target="_blank" rel="noopener">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/interchart/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/interchart">code</a>
          <a class="btn btn-code" href="https://huggingface.co/datasets/interchart/Interchart">data</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/interchart_slides.pdf" target="_blank">slides</a>
          
          
          
        </div>
      </div>
    </article>




<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Flowchart_poster.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.emnlp-main.1144.pdf" target="_blank" rel="noopener">Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents
</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
           Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Vivek Gupta, Dinesh Manocha
          <span class="venue"> 
          <a href="https://2025.emnlp.org/">EMNLP 2025</a>
          </span>
         
        </div>

        <p class="pub-abstract">
Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.emnlp-main.1144.pdf">paper</a>
          <a class="btn btn-paper" href="https://github.com/MananSuri27/FollowTheFlow">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/Flowchart_presentation.pdf" target="_blank">slides</a>
          
          
        

        </div>
      </div>
    </article>




<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/weaver_poster.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.emnlp-main.1436.pdf" target="_blank" rel="noopener">Weaver: Interweaving SQL and LLM for Table Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
          Rohit Khoja, Devanshu Gupta, Yanjie Fu, Dan Roth, Vivek Gupta
          <span class="venue"> 
          <a href="https://2025.emnlp.org/">EMNLP 2025</a>
          </span>
         
        </div>

        <p class="pub-abstract">
Querying tables with unstructured data is challenging due to the presence of text (or image), either embedded in the table or in external paragraphs, which traditional SQL struggles to process, especially for tasks requiring
 semantic reasoning. While Large Language
 Models (LLMs) excel at understanding context,
 they face limitations with long input sequences.
 Existing approaches that combine SQL and
 LLMs typically rely on rigid, predefined work
flows, limiting their adaptability to complex
 queries. To address these issues, we introduce
 Weaver , a modular pipeline that dynamically
 integrates SQL and LLMs for table-based question answering (TableQA). Weaver generates...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.emnlp-main.1436.pdf">paper</a>
         
          <a class="btn btn-paper" href="https://coral-lab-asu.github.io/weaver/">website</a> 
          <a class="btn btn-paper" href="https://github.com/CoRAL-ASU/weaver">code</a> 
          <a class="btn btn-slides" href="https://coral-lab-asu.github.io/weaver/presentation.pdf" target="_blank">slides</a>

        </div>
      </div>
    </article>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/TABARD_POSTER.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-emnlp.1189.pdf" target="_blank" rel="noopener">TABARD: A Novel Benchmark for Tabular Anomaly Analysis, Reasoning and Detection
</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
            Manan Roy Choudhury, Anirudh Iyengar, Shikhhar Siingh, Sugeeth Puranam, Vivek Gupta
          <span class="venue"> 
          <a href="https://2025.emnlp.org/">EMNLP 2025</a>
          </span>
         
        </div> 

        <p class="pub-abstract">
We study the capabilities of large language models (LLMs) in detecting fine-grained anomalies in tabular data. Specifically, we examine: (1) how well LLMs can identify diverse anomaly types—including factual, logical, temporal, and value-based errors; (2) the impact of prompt design and prompting strategies; and (3) the effect of table structure and anomaly type on detection accuracy. To this end, we introduce TABARD, a new benchmark constructed by perturbing tables from WikiTQ, FeTaQA, Spider, and BEAVER. The dataset spans multiple domains and eight anomaly categories, including paired clean and corrupted tables. We evaluate LLMs using direct, indirect, and Chain-of-Thought (CoT) prompting. Our results reveal notable limitations in standard prompting, especially for complex reasoning tasks and longer tables. To overcome these issues, we propose a unified framework combining multi-step prompting, self-verification, and constraint-based rule execution. Our approach...
          
        </p>

        <div class="pub-links">
            

          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-emnlp.1189.pdf">paper</a> 
          <a class="btn btn-paper" href="https://tabard-emnlp-2025.github.io">website</a> 
          <a class="btn btn-paper" href="https://github.com/TABARD-emnlp-2025/TABARD-dataset.git">data</a>
          <a class="btn btn-paper" href="https://github.com/TABARD-emnlp-2025/TABARD-code.git">code</a>  
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/TABARD_presentation.pdf" target="_blank">slides</a>

        </div>
      </div>
    </article>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Federated_RAG_Poster.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-emnlp.388.pdf" target="_blank" rel="noopener">Federated Retrieval-Augmented Generation: A Systematic Mapping Study
</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
            Abhijit Chakraborty, Chahana Dahal, Vivek Gupta
          <span class="venue"> 
          <a href="https://2025.emnlp.org/">EMNLP 2025</a> (Findings)
          
          </span>
         
        </div>

        <p class="pub-abstract">
Federated Retrieval-Augmented Generation (Federated RAG) combines Federated Learning (FL),which enables distributed model training without exposing raw data, with Retrieval-Augmented Generation (RAG), which improves the factual accuracy of language models by grounding outputs in external knowledge. As large language models are increasingly deployed in privacy-sensitive domains such as healthcare, finance, and personalized assistance, Federated RAG offers a promising framework for secure, knowledge-intensive natural language processing (NLP). To the best of our knowledge, this paper presents the first systematic mapping study of Federated RAG, covering literature published between 2020 and 2025. Following Kitchenham’s...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-emnlp.388.pdf">paper</a>
          
        

        </div>
      </div>
    </article>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/M_Help_placeholder.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-emnlp.1225.pdf" target="_blank" rel="noopener">M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals
</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
            MSVPJ Sathvik, Zuhair Hasan Shaik, Vivek Gupta
          <span class="venue"> 
          <a href="https://2025.emnlp.org/">EMNLP 2025</a> (Findings)
          </span>
         
        </div>

        <p class="pub-abstract">
Mental health disorders are a global crisis.
 While various datasets exist for detecting such
 disorders, there remains a critical gap in identifying individuals actively seeking help. This
 paper introduces a novel dataset, M-HELP,
 specifically designed to detect help-seeking
 behavior on social media. The dataset goes
 beyond traditional labels by identifying not
 only help-seeking activity but also specific
 mental health disorders and their underlying
 causes, such as relationship challenges or financial stressors. AI models...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-emnlp.1225.pdf">paper</a>
          <a class="btn btn-data" href="https://huggingface.co/datasets/zuhashaik/M-Help">data</a>
          
          
          
        

        </div>
      </div>
    </article>



    <!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MapIQ_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2507.11625" target="_blank" rel="noopener">MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
            V Srivastava, F Lei, S Mukhopadhyay, V Gupta, R Maciejewski
          </span>
          <span class="venue">
             <a href="https://colmweb.org/" target="_blank" rel="noopener">COLM 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          Recent advancements in multimodal large language models (MLLMs) have
 driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to
 visual question answering with maps (Map-VQA). However, Map-VQA
 research has primarily focused on choropleth maps, which cover only a
 limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising
 14,706 question-answer pairs across three map types—choropleth maps,
 cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another
 and a human baseline. An additional experiment...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2507.11625" target="_blank" rel="noopener">paper</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/TabXEval_Poster.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-acl.1176.pdf" target="_blank" rel="noopener">TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
          Vihang Pancholi, Jainit Sushil Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta
          <span class="venue"> 
          <a href="https://2025.aclweb.org/">ACL 2025</a>
          </span>
         
        </div>

        <p class="pub-abstract">
 Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard
 metrics often overlook subtle structural and
 content-level discrepancies. To address this,
 we propose a rubric-based evaluation frame
work that integrates multi-level structural descriptors with fine-grained contextual signals,
 enabling more precise and consistent table
 comparison. Building on this, we introduce
 TabXEval, an eXhaustive and eXplainable
 two-phase evaluation framework. TabXEval first aligns reference and predicted...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-acl.1176.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/tabxeval/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TabXEval">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/TabXEval_presentation.pdf" target="_blank">slides</a>

        </div>
      </div>
    </article>

    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Map&Make-Poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-long.1460.pdf" target="_blank" rel="noopener">Map&Make: Schema Guided Text to Table Generation</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Naman Ahuja, Fenil Bardoliya, Chitta Baral, Vivek Gupta</span>
          <span class="venue">
          <a href="https://2025.aclweb.org/">ACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          Transforming dense, unstructured text into interpretable tables—commonly referred to as Text-to-Table generation—is a key task in information extraction. Existing methods often overlook what complex information to extract and how to infer it from text. We present Map&Make, a versatile approach that decomposes text into atomic propositions to infer latent schemas, which are then used to generate tables capturing both qualitative nuances and quantitative facts. We evaluate...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-long.1460.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/map-make/">website</a>
          <a class="btn btn-code" href="https://github.com/coral-lab-asu/map-make/">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/Map&Make_presentation.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/PRAISE_Poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-demo.62.pdf" target="_blank" rel="noopener">PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.aclweb.org/">ACL 2025</a>(Demo)
              </span>
        </div>

        <p class="pub-abstract">
          Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-demo.62.pdf">paper</a>
          <a class="btn btn-site" href="https://project-praise.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/project-PRAISE/system-demo/">code</a>

        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/LLM_Symbolic_Poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-acl.1022.pdf" target="_blank" rel="noopener">LLM-Symbolic Integration for Robust Temporal Tabular Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Atharv Kulkarni, Kushagra Dixit, Vivek Srikumar, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Temporal tabular question answering presents a significant challenge for Large Language Models (LLMs), requiring robust reasoning over structured data—a task where traditional prompting methods often fall short. These methods face challenges such as memorization, sensitivity to table size, and reduced performance on complex queries. To overcome these limitations, we introduce TEMPTABQA-C, a synthetic dataset designed for systematic and controlled evaluations, alongside a symbolic intermediate representation that transforms tables into database schemas. This structured approach allows LLMs to generate and execute SQL queries, enhancing generalization and mitigating biases. By incorporating...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-acl.1022.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/llm_symbolic/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TEMPTABQA-C/tree/main/Dataset_Creation">code</a>
          <a class="btn btn-data" href="https://github.com/CoRAL-ASU/TEMPTABQA-C">data</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/LLM-Symbolic_presentation.pdf" target="_blank">slides</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Getreason_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-long.1439.pdf" target="_blank" rel="noopener">GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Publicly significant images from events carry
 valuable contextual information with applications in domains such as journalism and education. However, existing methodologies of ten struggle to accurately extract this contextual relevance from images. To address
 this challenge, we introduce GETREASON(Geospatial Event Temporal Reasoning), a
 framework designed to go beyond surface
level image descriptions and infer deeper con
textual meaning. We hypothesize that extracting global event, temporal, and geospatial information from an image enables a
 more accurate understanding of its contextual significance. We also introduce a new
 metric GREAT (Geospatial, Reasoning and
 Event Accuracy with Temporal alignment) for
 a reasoning capturing evaluation. Our layered multi-agentic approach, evaluated...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-long.1439.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/getreason/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/getreason">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/Getreason_presentation.pdf" target="_blank">slides</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MapWise_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.473.pdf" target="_blank" rel="noopener">MAPWise: Evaluating Vision-Language Models for Advanced Map Queries</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Manish Shrivastava, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
            
             Vision-language models (VLMs) excel at tasks
 requiring joint understanding of visual and linguistic information. A particularly promising
 yet under-explored application for these models
 lies in answering questions based on various
 kinds of maps. This study investigates the
 efficacy of VLMs in answering questions based
 on choropleth maps, which are widely used for
 data analysis and representation. To facilitate
 and encourage research in this area, we introduce a novel map-based question-answering
 benchmark, consisting of maps from three
 geographical regions (United States, India,
 China), each containing 1000 questions. Our
 benchmark...
            
            
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.473.pdf">paper</a>
          <a class="btn btn-site" href="https://map-wise.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/abhishekrajgaria/Mapwise">code</a>
          <a class="btn btn-data" href="https://github.com/map-wise/mapwise-dataset">data</a>
           <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/MapWise_slides.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Infotabs_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.329.pdf" target="_blank" rel="noopener">Leveraging LLM for Synchronizing Information Across Multilingual Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.329.pdf">paper</a>
          <a class="btn btn-site" href="https://zero-shot-llm-infosync.github.io/zero-shot-llm-infosync/">website</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/LLM_infosync_slides.pdf" target="_blank">slides</a>
          
          

          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/TRANSIENTTABLES_Poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.332.pdf" target="_blank" rel="noopener">TRANSIENT TABLES: Evaluating LLMs’ Reasoning on Temporally Evolving Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Humans continuously make new discoveries,
 and understanding temporal sequence of events
 leading to these breakthroughs is essential for
 advancing science and society. This ability to
 reason over time allows us to identify future
 steps and understand the effects of financial
 and political decisions on our lives. However,
 large language models (LLMs) are typically
 trained on static datasets, limiting their ability
 to perform effective temporal reasoning. To
 assess the temporal reasoning capabilities of
 LLMs, we present the TRANSIENTTABLES
 dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238
 entities across multiple time periods. We introduce a template-based question-generation
 pipeline that harnesses LLMs to refine both
 templates and questions. Additionally, we establish baseline...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.332.pdf">paper</a>
          <a class="btn btn-site" href="https://transienttables.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/harsh1399/TransientTables">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/TRANSIENTTABLES_presentation.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/HStar_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.445.pdf" target="_blank" rel="noopener">H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Nikhil Abhyankar, Vivek Gupta, Dan Roth, Chandan Reddy</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Tabular reasoning involves interpreting natural language queries about tabular data, which presents a unique challenge of combining language understanding with structured data analysis. Existing methods employ either textual reasoning, which excels in semantic interpretation but struggles with mathematical
 operations, or symbolic reasoning, which handles computations well but lacks semantic understanding. This paper introduces a novel
 algorithm H-STAR that integrates both symbolic and semantic (textual) approaches in a
 two-stage process to address these limitations.
 H-STAR employs: (1) step-wise table extraction using ‘multi-view’ column retrieval followed by row extraction, and (2) adaptive reasoning that adapts reasoning strategies based on question types, utilizing semantic reasoning for direct lookup and complex lexical queries while augmenting textual reasoning with symbolic reasoning support for quantitative and logical tasks. Our extensive...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.445.pdf">paper</a>
          <a class="btn btn-site" href="https://hstar-llm.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/nikhilsab/H-STAR">code</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/HStar_slides.pdf" target="_blank">slides</a>
         
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/CLEAR_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-naacl.278.pdf" target="_blank" rel="noopener">Enhancing Temporal Understanding in LLMs for Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Irwin Deng, Kushagra Dixit, Dan Roth, Vivek Gupta</span>
          
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a> (Findings)
              </span>
        </div>

        <p class="pub-abstract">
           Temporal reasoning over tabular data presents
 substantial challenges for large language models (LLMs), as evidenced by recent research. In
 this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific
 limitations of LLMs. Our investigation leads
 to enhancements in TempTabQA, a benchmark
 specifically designed for tabular temporal question answering. We provide critical insights
 for enhancing LLM performance in temporal
 reasoning tasks with tabular data. Furthermore,
 we introduce a novel approach, C.L.E.A.R to
 strengthen LLM capabilities in this domain.
 Our findings demonstrate that our method im
proves evidence-based reasoning across various
 models. Additionally, our experimental...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-naacl.278.pdf">paper</a>
          <a class="btn btn-site" href="https://clear-prompting.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/clear-temptabqa/clear_temptabqa">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/ntsebench_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-naacl.204.pdf" target="_blank" rel="noopener">NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Pranshu Pandya, Vatsal Gupta, Agney S Talwarr, Tushar Kataria, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
          
        </div>

        <p class="pub-abstract">
           Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand
 the ability to quickly reason, decipher, and
 evaluate patterns both textually and spatially.
 Due to extensive training on vast amounts of
 human-curated data, large language models
 (LLMs) and vision language models (VLMs)
 excel in common-sense reasoning tasks, but
 still struggle with more complex reasoning
 that demands deeper cognitive understanding.
 We introduce NTSEBENCH, a new dataset designed to evaluate cognitive multimodal reasoning and problem-solving skills of large models. The dataset contains 2,728 multiple-choice
 questions, accompanied by a total of 4,642 images, spanning 26 categories....
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-naacl.204.pdf">paper</a>
          <a class="btn btn-site" href="https://ntsebench.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/NTSEBench/NTSEBench">code</a>
          <a class="btn btn-data" href="https://github.com/NTSEBench/NTSEBench">data</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/ntsebench_slides.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/msin_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.emnlp-main.1237.pdf" target="_blank" rel="noopener">Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Vatsal Gupta, Pranshu Pandya, Tushar Kataria, <b>Vivek Gupta</b>, Dan Roth <br></span>
          <span class="venue">
              <a href="https://2024.emnlp.org/">EMNLP 2024</a>
             </span>
        </div>

        <p class="pub-abstract">
          Language models, characterized by their black
box nature, often hallucinate and display sensitivity to input perturbations, causing concerns
 about trust. To enhance trust, it is imperative
 to gain a comprehensive understanding of the
 model’s failure modes and develop effective
 strategies to improve their performance. In this
 study, we introduce a methodology designed
 to examine how input perturbations affect language models across various scales, including
 pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance
 the model’s robustness to input perturbations.
 Additionally, ...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.emnlp-main.1237.pdf">paper</a>
          <a class="btn btn-site" href="https://msin-infotabs.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/msin-infotabs/Evaluating-Concurrent-Robustness-of-Language-Models-Across-Diverse-Challenge-Sets">code</a>
           <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/msin_slides.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Unravelling_truth_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.973.pdf" target="_blank" rel="noopener">Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"><br>
  Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, <b>Vivek Gupta</b>, Dan Roth <br></span>
          
          <span class="venue">
          <a href="https://2024.emnlp.org/">EMNLP 2024</a> (Findings)
          </span>
        </div>

        <p class="pub-abstract">
          Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally,...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.973.pdf">paper</a>
          <a class="btn btn-site" href="https://robustcqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/RobustCQA/Robust-CQA">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MMTabQA_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.822.pdf" target="_blank" rel="noopener">Knowledge-Aware Reasoning over Multimodal Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, <b>Vivek Gupta</b>, Mohit Bansal, Dan Roth</span>
          <span class="venue">
              <a href="https://2024.emnlp.org/">EMNLP 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
           Existing datasets for tabular question answering typically focus exclusively on text within
 cells. However, real-world data is inherently
 multimodal, often blending images such as
 symbols, faces, icons, patterns, and charts
 with textual content in tables. With the evolution of AI models capable of multimodal
 reasoning, it is pertinent to assess their efficacy in handling such structured data. This
 study investigates whether current AI models
 can perform knowledge-aware reasoning on
 multimodal structured data. We explore their
 ability to reason on tables that integrate both
 images and text, introducing MMTABQA, a
 new dataset designed for this purpose. Our
 experiments...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.822.pdf">paper</a>
          <a class="btn btn-site" href="https://mmtabqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/MMTabQA/mmtabqa">code</a>
          <a class="btn btn-data" href="https://github.com/MMTabQA/mmtabqa">data</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/mmtabqa_slides.pdf" target="_blank">slides</a>
        </div>
      </div>
    </article>


<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/FlowVQA_poster.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.78.pdf" target="_blank" rel="noopener">FlowVQA: Mapping Multimodal Logic in
 Visual Question Answering with Flowcharts</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, <b>Vivek Gupta</b>, Dan Roth</span>
          <span class="venue">
              <a href="https://2024.aclweb.org/">ACL 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
            Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning
 skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language
 models in reasoning with flowcharts as visual
 contexts. FlowVQA comprises 2,272 carefully
 generated and human-verified flowchart images
 from three distinct content sources, along with
 22,413 diverse question-answer pairs, to test a
 spectrum of reasoning tasks, including information localization, decision-making, and logical
 progression. We conduct a thorough baseline
 evaluation on a suite of both open-source and
 proprietary multimodal language models using
 various strategies, followed by an analysis of
 directional bias. The results...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.78.pdf">paper</a>
          <a class="btn btn-site" href="https://flowvqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/flowvqa">code</a>
          <a class="btn btn-data" href="https://github.com/flowvqa/flowvqa">data</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/FlowVQA_slides.pdf" target="_blank">slides</a>
          
        </div>
      </div>
      </article>


<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Evaluating_LM_Math_Reasoning_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.231.pdf" target="_blank" rel="noopener">Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Pragya Srivastava, Manuj Malik, <b>Vivek Gupta</b>, Tanuja Ganu, Dan Roth</span>
          <span class="venue">
              <a href="https://2024.aclweb.org/">ACL 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
            Large Language Models (LLMs), excel in
 natural language understanding, but their capability for complex mathematical reasoning
 with a hybrid of structured tables and unstructured text remain uncertain. This study explores LLMs’ mathematical reasoning on four
 financial tabular question-answering datasets:
 TATQA, FinQA, ConvFinQA, and Multihiertt.
 Through extensive experiments with various
 models and prompting techniques, we assess
 how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to
 table complexity and performance variations
 with an increasing number of arithmetic reasoning steps. The results...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.231.pdf">paper</a>
          <a class="btn btn-slides" href="{{ site.baseurl }}/presentation_docs/EEDP_presentation.pdf" target="_blank">slides</a>
          
        </div>
      </div>
    </article>
    
<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/ChartCheck_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.828.pdf" target="_blank" rel="noopener">ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World Chart Images</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Mubashara Akhtar, Nikesh Subedi, <b>Vivek Gupta</b>, Sahar Tahmasebi, Oana Cocarascu, Elena Simperl</span>
          <span class="venue">
              <a href="https://2024.aclweb.org/">ACL 2024</a> (Findings)
           </span>  
        </div>

        <p class="pub-abstract">
           Whilst fact verification has attracted substantial interest in the natural language processing
 community, verifying misinforming statements
 against data visualizations such as charts has
 so far been overlooked. Charts are commonly
 used in the real-world to summarize and communicate key information, but they can also
 be easily misused to spread misinformation
 and promote certain agendas. In this paper,
 we introduce ChartCheck, a novel, large-scale
 dataset for explainable fact-checking against
 Chart:
 Evidence
 real-world charts, consisting of 1.7k charts and
 10.5k human-written claims and explanations.
 We systematically...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.828.pdf">paper</a>
          <a class="btn btn-code" href="https://github.com/mubasharaak/ChartCheck">code</a>
        </div>
      </div>
    </article>
    
<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/Enhancing_Question_Answering_on_Charts_QA_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.blackboxnlp-1.11.pdf" target="_blank" rel="noopener">Enhancing Question Answering on Charts Through Effective Pre-training Tasks</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Ashim Gupta, <b>Vivek Gupta</b>, Shuo Zhang, Yujie He, Ning Zhang, Shalin Shah </span>
          <span class="venue">
              <a href="https://blackboxnlp.github.io/">BlackboxNLP 2024</a> (Findings)
              
           </span>  
        </div>

        <p class="pub-abstract">
            To completely understand a document, the use
 of textual information is not enough. Under
standing visual cues, such as layouts and charts,
 is also required. While the current state-of
the-art approaches for document understanding
 (both OCR-based and OCR-free) work well,
 we have not found any other works conducting a thorough analysis of their capabilities and
 limitations. Therefore, in this work, we address the limitation of current VisualQA models when applied to charts and plots. To investigate shortcomings of the state-of-the-art models, we conduct a comprehensive behavioral analysis, using ChartQA as a case study.
 Our findings ...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.blackboxnlp-1.11.pdf">paper</a>
          
        </div>
      </div>
    </article>
<hr>

<section class="past-publications">

     <h2 class="section-title">PAST PUBLICATIONS</h2>
   
  <li><b>TempTabQA: Temporal Question Answering for Semi-Structured Tables</b><br>
  <b>Vivek Gupta</b>, Pranshu Kandoi, Mahek Bhavesh Vora, Shuo Zhang, Yujie He, Ridho Reinanda, Vivek Srikumar <br>
  Published at <a href="https://2023.emnlp.org/">EMNLP 2023</a>, <a href="https://aclanthology.org/2023.emnlp-main.149.pdf">Paper</a>, 
    <a href="https://temptabqa.github.io/">Project Page</a>
  </li>

  <li><b>Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data</b><br>
  Mubashara Akhtar, Abhilash Shankarampeta, <b>Vivek Gupta</b>, Arpit Patil, Oana Cocarascu, Elena Simperl <br>
  Published at <a href="https://2023.emnlp.org/">EMNLP 2023(Findings)</a>, <a href="https://aclanthology.org/2023.findings-emnlp.1028.pdf">Paper</a>
  </li>

  <li><b>InfoSync: Information Synchronization across Multilingual Semi-structured Tables</b><br>
  Sidharth Khincha, Chelsi Jain, <b>Vivek Gupta</b>, Tushar Kataria, Shuo Zhang <br>
  Published at <a href="https://2023.aclweb.org/">ACL 2023</a>, presented at <a href="https://megagon.ai/matching-2023/">Matching@ACL 2023</a> <a href="https://info-sync.github.io/info-sync/">Project Page</a>, <a href="https://aclanthology.org/2023.findings-acl.159/">Paper</a>, <a href="https://drive.google.com/file/d/18b67cU2XDxMZpJ-99VdWeL8F3lBGyaLj/view">Video</a>, <a href="https://vgupta123.github.io/docs/infosync_poster.pdf">Poster</a>, <a href="https://docs.google.com/presentation/d/1lPm7c8hubwADNpWHfcqNCRX-KlChh8gEzWPBLIFP79Y/edit?usp=sharing">PPT</a>
  </li>

  <li><b>Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning</b>, <br/> 
    <b>Vivek Gupta</b>, Shuo Zhang, Alakananda Vempala, Yujie He, Temma Choji, Vivek Srikumar <br/> published at 
    <a href="https://www.2022.aclweb.org/">ACL 2022</a> <a href="https://aclanthology.org/2022.acl-long.231.pdf">Paper</a>
    <a href="docs/ACL-2022_Poster.pdf">Poster</a> [<a href="docs/ACL-2022-PPT.pptx">PPT</a>]
    <a href="https://youtu.be/YwGik-QCntA">Video</a>
    <a href="https://www.bloomberg.com/company/stories/bloomberg-ai-group-cto-office-publish-8-research-papers-at-acl-2022-dublin/">Media</a>
    <a href="https://www.linkedin.com/posts/shawn-edwards-0902a28b_bloombergs-ai-group-cto-office-publish-activity-6935303372510027776-SbHp?utm_source=share&amp;utm_medium=member_desktop">LinkedIn</a>
</li>

<li><b>Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning</b>, <br /> <b>Vivek Gupta</b>, Riyaz A. Bhat, Atreya Ghosal, Manish Srivastava, Maneesh Singh, Vivek Srikumar <br /> published at <a href="https://transacl.org/index.php/tacl">TACL 2022</a>, presented at <a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="https://aclanthology.org/2022.tacl-1.38.pdf">Paper</a>][<a href="https://arxiv.org/pdf/2108.00578.pdf">Preprint</a>] [<a href="docs/TACL-2022_Poster-Final.pdf">Poster</a>] [<a href="docs/TACL-2022_PPT.pdf">PPT</a>] [<a href="https://youtu.be/VRvttI4Ppso">Video</a>]
</li>

<li><b>Bilingual Tabular Inference: A Case Study on Indic Languages</b> <br /> Chaitanya Agarwal*, <b>Vivek Gupta</b>*, Anoop Kunchukuttan, Manish Shrivastava <br /> published at <a href="https://2022.naacl.org">NAACL 2022</a> [<a href="https://aclanthology.org/2022.naacl-main.295.pdf">Paper</a>] [<a href="docs/NAACL2022.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1OEhEcHhr8NlSAutqJhYQc4D_0oW0M5AIAGql5hoE31I/edit?usp=sharing">PPT</a>] [<a href="https://docs.google.com/presentation/d/1kHmZ1r2a8iA7tdCTS4ISDbEGFeAFJwyPTX1tOkCTi3M/edit?usp=sharing">Poster</a>] [<a href="https://www.youtube.com/watch?v=NXGUYl_en98&amp;ab_channel=ChaitanyaAgarwal">Video</a>]
</li>

<li><b>Trans-KBLSTM: An External Knowledge Enhanced Transformer BiLSTM model for Tabular Reasoning</b>, <br /> Yerram Varun*, Aayush Sharma*, <b>Vivek Gupta</b>* <br /> to appear at <a href="https://sites.google.com/view/deelio-ws">DeeLIO-2022</a> @<a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="https://aclanthology.org/2022.deelio-1.7.pdf">Paper</a>] [<a href="docs/TransKBLSTM.pdf">Preprint</a>] [<a href="docs/Trans-KBLSTM_Poster.pdf">Poster</a>] [<a href="docs/Trans-KBLSTM_PPT.pdf">PPT</a>] [<a href="https://youtu.be/CldBchM8IK0">Video</a>] <br />
<tt>Won Best Paper award at <a href="https://sites.google.com/view/deelio-ws">DeeLIO-2022</a></tt>
</li>

<li><b>XInfoTabS: Evaluating Multilingual Tabular Natural Language Inference</b>, <br />  Bhavnick Minhas*, Anant Shankhdhar*, <b>Vivek Gupta</b>*, Divyanshu Aggarwal, Shuo Zhang,<br /> published at <a href="https://mml-workshop.github.io">MML-2022</a> (non-archival) and <a href="https://fever.ai">FEVER-2022</a> (archival) @<a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="docs/XInfoTabS.pdf">Preprint</a>] [<a href="docs/XInfoTabS_Poster.pdf">Poster</a>] [<a href="docs/XInfoTabS_PPT.pdf">PPT</a>] [<a href="https://youtu.be/A9aS7sXY3BA">Video</a>] [<a href="https://www.bloomberg.com/company/stories/bloomberg-ai-group-cto-office-publish-8-research-papers-at-acl-2022-dublin/">Media</a>] [<a href="https://www.linkedin.com/posts/shawn-edwards-0902a28b_bloombergs-ai-group-cto-office-publish-activity-6935303372510027776-SbHp?utm_source=share&amp;utm_medium=member_desktop">LinkedIn</a>]
</li>

<li><b>Enhancing Tabular Reasoning with Pattern Exploiting Training</b>,<br /> Abhilash Shankarampeta*,  <b>Vivek Gupta</b>*, Shuo Zhang <br /> to appear at <a href="https://suki-workshop.github.io/">SUKI-2022</a> (non-archival) [<a href="https://suki-workshop.github.io/assets/paper/24.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1XmfE0YGlQ3wBTtHfAYi2edjaqk6j-LXTHNq2YS1lbtY/edit?usp=sharing">PPT</a>] [<a href="https://drive.google.com/file/d/12ex4MOIzR6soRZDLcRLkp8sFy0G0XlOg/view?usp=sharing">Poster</a>] [<a href="https://youtu.be/fHwaoawMg5s">Video</a>]<br /> (Extended Version at <a href="https://www.aacl2022.org/">AACL 2022</a>) [<a href="https://vgupta123.github.io/docs/pettable.pdf">Paper</a>] [<a href="https://infoadapet.github.io/">Project Page</a>] [<a href="https://www.bloomberg.com/company/stories/bloombergs-ai-engineering-group-publishes-4-nlp-research-papers-at-aacl-ijcnlp-2022/">Media</a>]
</li>

<li><b>Efficient Realistic Data Generation Framework for Semi-Structured Tabular Inference</b>, <br />
Dibyakanti Kumar*, <b>Vivek Gupta</b>*, Soumya Sharma, Shuo Zhang <br /> to appear at <a href="https://suki-workshop.github.io/">SUKI-2022</a>(non-archival)  [<a href="https://suki-workshop.github.io/assets/paper/25.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1WfB3Rx4m81d4T8u9QVTsKeonCoOZrzwG6RN0JyXi9pU/edit?usp=sharing">PPT</a>] [<a href="https://www.youtube.com/watch?v=6gDzTIYH6Rw&amp;feature=youtu.be&amp;ab_channel=DibyakantiKumar">Video</a>] [<a href="https://drive.google.com/file/d/1sTvTcLgoqyJP2cAXsSM0t7j9sfUWI9oO/view?usp=sharing">Poster</a>]<br /> (Extended Version at <a href="https://2022.emnlp.org/">EMNLP 2022</a>) [<a href="https://autotnli.github.io/">Project Page</a>] [<a href="https://vgupta123.github.io/docs/autotnli.pdf">Paper</a>] [<a href="https://www.bloomberg.com/company/stories/bloombergs-ai-engineering-group-cto-publish-5-nlp-research-papers-at-emnlp-2022/?linkId=192716295">Media</a>]
</li>

<li><b>Leveraging Data Recasting to Enhance Tabular Reasoning</b>,<br />
Aashna Jena*,  <b>Vivek Gupta</b>*, Manish Shrivastava, Julian Martin Eisenschlos <br /> to appear at <a href="https://suki-workshop.github.io/">SUKI-2022</a> (non-archival) [<a href="https://suki-workshop.github.io/assets/paper/26.pdf">Preprint</a>] [<a href="https://docs.google.com/presentation/d/1nOA-pmfP75GDhF8EgC9nagcy4j70otpLTHYpYCfRa2w/edit?usp=sharing">Poster</a>] [<a href="https://docs.google.com/presentation/d/1XtLoH7rt5phfSgR8kwadt9o-_7j2PUl2MOaKlgDgVb4/edit?usp=sharing">PPT</a>] [<a href="https://youtu.be/hZzvrYy3-3g">Video</a>] <br /> (Extended Version at <a href="https://2022.emnlp.org/">EMNLP 2022</a>) [<a href="https://recasting-to-nli.github.io/">Project Page</a>] [<a href="https://vgupta123.github.io/docs/recasting.pdf">Paper</a>] [<a href="https://ai.googleblog.com/2022/12/google-at-emnlp-2022.html">Media</a>] [<a href="https://docs.google.com/presentation/d/1ZClxfBJsKSc8a0NIojsOg-wWkxtw-WauI15bAT6ly5w/edit?usp=sharing">Poster</a>]
</li>

<li><b>RetroNLU: Retrieval Augmented Task Oriented Semantic Parsing</b>,<br />  <b>Vivek Gupta</b>, Akshat Shrivastava, Adithya Sagar, Armen Aghajanyan, Denis Savenkov,<br /> to appear at <a href="https://www.semiparametric.ml">Spa-NLP-2022</a> (non-archival) and <a href="https://sites.google.com/view/4thnlp4convai">NLP4ConvAI-2022</a> (archival) @<a href="https://www.2022.aclweb.org/">ACL 2022</a> [<a href="https://aclanthology.org/2022.nlp4convai-1.15.pdf">Paper</a>] [<a href="docs/RetroNLU.pdf">Preprint</a>] [<a href="docs/RetroNLU_Poster.pdf">Poster</a>] [<a href="docs/RetroNLU_PPT.pdf">PPT</a>] [<a href="https://youtu.be/sAsaTTveCmE">Video</a>]<br />
<tt>Won Outstanding Paper award at <a href="https://sites.google.com/view/4thnlp4convai">NLP4ConvAI-2022</a></tt>
</li>

<li><b>TabPert: An Effective Platform for Tabular Perturbation</b>, <br /> Nupur Jain,<b>Vivek Gupta</b>, Anshul Rai, Gaurav Kumar <br /> Published at <a href="https://2021.emnlp.org/">EMNLP 2021</a>, Demo track [<a href="https://aclanthology.org/2021.emnlp-demo.39.pdf">Paper</a>] [<a href="https://tabpert.github.io">Project Page</a>][<a href="https://arxiv.org/pdf/2108.00603.pdf">Preprint</a>] [<a href="docs/tabpert_ppt.pdf">PPT</a>] [<a href="https://www.youtube.com/watch?v=zjfRk8--jEY&amp;ab_channel=VivekGupta">Video</a>] [<a href="https://github.com/utahnlp/tabpert">Code</a>] 
</li>

<li><b>Incorporating External Knowledge to Enhance Tabular Reasoning</b>, <br /> J. Neeraja*, <b>Vivek Gupta</b>*, and Vivek Srikumar <br /> Published at <a href="https://2021.naacl.org/">NAACL 2021</a> [<a href="https://www.aclweb.org/anthology/2021.naacl-main.224.pdf">Paper</a>] [<a href="https://knowledge-infotabs.github.io">Project Page</a>] [<a href="https://github.com/utahnlp/knowledge_infotabs">Code</a>] [<a href="https://screencast-o-matic.com/watch/crhlnpVfYcG">Video</a>] [<a href="docs/NAACL_2021_Poster.pdf">Poster</a>] [<a href="https://docs.google.com/presentation/d/1MfkHFcfnoCiBY9ftp8Uw_uosz_Xo5UQNkZEwJZ-HS10/edit?usp=sharing">PPT</a>]
</li>

<li><b>InfoTabS: Inference on Tables as Semi-structured Data</b>, <br /> <b>Vivek Gupta</b>, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar <br /> Published at <a href="https://acl2020.org">ACL 2020</a> [<a href="https://www.aclweb.org/anthology/2020.acl-main.210/">Paper</a>] [<a href="https://infotabs.github.io">Project Page</a>] [<a href="https://www.youtube.com/watch?v=YhfU1BON8EI">Video</a>] [<a href="https://github.com/infotabs/infotabs">Data</a>] [<a href="https://github.com/utahnlp/infotabs-code">Code</a>]
</li>

  <li><b>IndicSemParse: Evaluating Inter-Bilingual Semantic Parsing for Indian Languages</b><br>
  Divyanshu Aggarwal*, <b>Vivek Gupta</b>*, Anoop Kunchukuttan <br>
  To appear at <a href="https://sites.google.com/view/5thnlp4convai/">NLP4ConvAI 2023</a> <a href="https://iesemparse.github.io/">Project Page</a>, <a href="https://arxiv.org/pdf/2304.13005.pdf">Preprint</a>
  </li>
  
  <li><b>IndicXNLI: Evaluating Multilingual Inference for Indian Languages</b><br>
  Divyanshu Aggarwal*, <b>Vivek Gupta</b>*, Anoop Kunchukuttan <br>
  To appear at <a href="https://mia-workshop.github.io/">MIA-2022</a> (non-archival) <a href="https://arxiv.org/pdf/2204.08776.pdf">Preprint</a>
  </li>

  <li><b>Logic Driven Classification for Low Resource Settings</b><br>
  Shagun Uppal, <b>Vivek Gupta</b>, Avinash Swaminathan, Debanjan Mahata, Rakesh Gosangi, Haimin Zhang, Rajiv Ratn Shah, Amanda Stent <br>
  Published at AACL-IJCNLP 2020 <a href="https://www.aclweb.org/anthology/2020.aacl-main.71.pdf">Paper</a>
  </li>

  <li><b>A Logic-Driven Framework for Consistency of Neural Models</b><br>
  Tao Li, <b>Vivek Gupta</b>, Maitrey Mehta, and Vivek Srikumar <br>
  Published at EMNLP-IJCNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1405.pdf">Paper</a>
  </li>

  <li><b>Unbiasing Review Ratings with Tendency-based Collaborative Filtering</b><br>
  Pranshi Yadav*, Priya Yadav*, Pegah Nokhiz, <b>Vivek Gupta</b><br>
  Published at <a href="https://aacl2020-srw.github.io">AACL-IJCNLP SRW 2020</a> <a href="https://www.aclweb.org/anthology/2020.aacl-srw.8.pdf">Paper</a>
  </li>

  <li><b>User Bias Removal in Review Score Prediction</b><br>
  Rahul Wadbude, <b>Vivek Gupta</b>, Dheeraj Mekala, Harish Karnick <br>
  Published at CoDS-COMAD 2018 and DAB@CIKM 2017 <a href="https://dl.acm.org/citation.cfm?id=3152520">Paper</a>
  </li>

  <li><b>Equalizing Recourse across Groups</b><br>
  <b>Vivek Gupta</b>*, Pegah Nokhiz*, Chitradeep Dutta Roy*, Suresh Venkatasubramanian <br>
  Technical Report. <a href="https://arxiv.org/abs/1909.03166">Preprint</a>
  </li>

  <li><b>Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles</b> <br /> Dhruv Mahajan, <b>Vivek Gupta</b>, Satya Keerthi, Sundararjan Sellamanickam <br /> Technical Report. [<a href="https://arxiv.org/abs/1711.05482">Preprint</a>]
</li>

<li><b>Unsupervised Contextualized Document Representation</b>, <br /> Ankur Gupta, <b>Vivek Gupta</b> <br /> Published at <a href="https://sites.google.com/view/sustainlp2021/call-for-papers">SustaiNLP 2021</a> at <a href="https://2021.emnlp.org/">EMNLP 2021</a> workshop. [<a href="https://aclanthology.org/2021.sustainlp-1.17.pdf">Paper</a>] [<a href="https://arxiv.org/pdf/2109.10509.pdf">Preprint</a>] [<a href="docs/sustainlp21_ppt.pdf">PPT</a>] [<a href="docs/sustainlp21_poster.pdf">Poster</a>] [<a href="https://www.youtube.com/watch?v=xp576-9fZGw&amp;ab_channel=VivekGupta">Video</a>] [<a href="https://github.com/vgupta123/contextualize_scdv">Code</a>]
</li>
  
<li><b>Improving Document Classification with Multi-Sense Embeddings</b>, <br /> <b>Vivek Gupta</b>, Ankit Saw,  Pegah Nokhiz, Harshit Gupta, and Partha Talukdar <br /> Published at <a href="http://ecai2020.eu">ECAI 2020</a> [<a href="https://ecai2020.eu/papers/391_paper.pdf">Paper</a>] [<a href="http://vivgupt.blogspot.com/2019/06/document-vector-estimation-using.html">Blog</a>] [<a href="https://www.youtube.com/watch?v=tKY5P8JFw3Q">Video</a>] [<a href="https://github.com/vgupta123/SCDV-MS">Code</a>]  <br />(extention of <a href="https://naacl2019-srw.github.io/">NAACL-SRW 2019</a> work)<br />
</li>
  
<li><b>P-SIF: Document Embeddings using Partition Averaging</b> <br /> <b>Vivek Gupta</b>, Ankit Saw, Pegah Nokhiz,  Praneeth Netrapalli, Piyush Rai, Partha Talukdar<br /> Published at <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>, Presented at <a href="https://sites.google.com/view/sustainlp2020">SustaiNLP 2020</a> [<a href="docs/AAAI-GuptaV.3656.pdf">Paper</a>] [<a href="docs/aaai2020appendix.pdf">Appendix</a>] [<a href="docs/AAAI20_PPT.pdf">PPT</a>] [<a href="docs/AAAI20_Poster.pdf">Poster</a>] [<a href="https://github.com/vgupta123/P-SIF">Code</a>] [<a href="http://vivgupt.blogspot.com/2019/06/document-vector-estimation-using.html">Blog</a>]
</li>
  
<li><b>Word Polysemy Aware Document Vector Estimation</b> <br /> <b>Vivek Gupta</b>, Ankit Saw, Harshit Gupta, Pegah Nokhiz and Partha Talukdar <br /> Presented at <a href="https://naacl2019-srw.github.io/">NAACL-SRW 2019</a> (non-archival) <br />(extended version appear at <a href="http://ecai2020.eu">ECAI 2020</a>) [<a href="https://github.com/vgupta123/SCDV-MS">Code</a>]
</li>
  
<li><b>Sparse Composite Document Vectors using soft clustering over distributional representations</b> <br /> Dheeraj Mekala*, <b>Vivek Gupta</b>*, Bhargavi Paranjape , Harish Karnick <br /> Published at <a href="https://www.aclweb.org/anthology/events/emnlp-2017/">EMNLP 2017</a>. [<a href="http://aclweb.org/anthology/D17-1069">Paper</a>] [<a href="https://www.slideshare.net/vivekgupta3150/sparse-composite-document-vector-emnlp-2017">PPT</a>] [<a href="https://vimeo.com/238235553">Video</a>] [<a href="https://github.com/dheeraj7596/SCDV">Code</a>]
</li>
  
<li><b>On Dimensional Linguistic Properties of the Word Embedding Space</b> <br /> Vikas Raunak*, Vaibhav Kumar*, <b>Vivek Gupta</b> and Florian Metze <br /> Presented at <a href="https://sites.google.com/view/acl19studentresearchworkshop/">ACL-SRW 2019</a> (non-archival), Published at <a href="https://sites.google.com/view/repl4nlp2020/home">RepL4NLP 2020</a> [<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.19.pdf">Paper</a>] [<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.19/">Paper</a>] [<a href="https://github.com/vyraun/dlp">Code</a>] <br />
</li>
  
<li><b>Effective Dimensionality Reduction for Word Embeddings</b> <br /> Vikas Raunak, <b>Vivek Gupta</b> and Florian Metze <br /> Published at <a href="https://sites.google.com/view/repl4nlp2019/home">RepL4NLP 2019</a>. [<a href="https://www.aclweb.org/anthology/W19-4328">Paper</a>] [<a href="docs/RepL4NLP_Poster.pdf">Poster</a>] [<a href="https://github.com/vyraun/Half-Size">Code</a>]
</li>
  
  <li><b>SumPubMed: Summarization Dataset of PubMed Scientific Articles</b><br>
  <b>Vivek Gupta</b>, Prerna Bharti, Pegah Nokhiz, Harish Karnick <br>
  Accepted to appear in ACL-IJCNLP SRW 2021 <a href="docs/121_paper.pdf">Preprint</a> <a href="https://github.com/vgupta123/sumpubmed">Dataset</a> <a href="https://docs.google.com/presentation/d/12zNpBw02DXjwQYTop5LQHwXe_HCLR8dg6D4bYYMYS7E/edit?usp=sharing">PPT</a>
  </li>

  <li><b>Unsupervised Semantic Abstractive Summarization</b><br>
  Shibhansh Dohare, <b>Vivek Gupta</b>, Harish Karnick <br>
  Published at ACL-SRW 2018 <a href="https://arxiv.org/pdf/1706.01678.pdf">Preprint</a> <a href="http://aclweb.org/anthology/P18-3011">Paper</a>
  </li>

  <li><b>Distributional Semantics meet Multi-Label Learning</b><br>
  <b>Vivek Gupta</b>, Rahul Wadbude, Nagararjan Natararjan, Harish Karnick, Prateek Jain, Piyush Rai <br>
  Published at AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4260">Paper</a>
  </li>

  <li><b>Bayes-optimal Hierarchical Classification over Asymmetric Tree-Distance Loss</b><br>
  Dheeraj Mekala, <b>Vivek Gupta</b>, Purushottam Kar, Harish Karnick <br>
  Technical Report. <a href="https://arxiv.org/pdf/1802.06771.pdf">Report</a>
  </li>

  <li><b>On Long-Tailed Phenomena in Neural Machine Translation</b>, <br /> Vikas Raunak, Siddharth Dalmia, <b>Vivek Gupta</b>, and Florian Metze <br /> Published at <a href="https://2020.emnlp.org">EMNLP 2020</a> (Findings), Presented at <a href="http://structuredprediction.github.io/SPNLP20">SPNLP2020</a> [<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.276.pdf">Paper</a>] [<a href="https://github.com/vyraun/long-tailed">Code</a>]
</li>

  <li><b>Product Classification in E-Commerce using Distributional Semantics</b><br>
  <b>Vivek Gupta</b>, Harish Karnick, Ashendra Bansal, Pradhuman Jhala <br>
  Published at COLING 2016 <a href="https://aclweb.org/anthology/C/C16/C16-1052.pdf">Paper</a>
  </li>

  <li><b>Assisting Humans to Achieve Optimal Sleep by Changing Ambient Temperature</b><br>
  <b>Vivek Gupta</b>*, Siddhant Mittal*, Sandip Bhaumik, Raj Roy <br>
  Published at BIBM 2016 <a href="http://ieeexplore.ieee.org/document/7822635/">Paper</a>
  </li>

<style>
/* Basic styling — tweak to match fonts/colors of your site */
.selected-publications {
  max-width: 980px;
  margin: 2rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;
}

  .past-publications {
  max-width: 980px;
  margin: 0.8rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;

}
.section-title {
  font-size: 1.5rem;
  font-weight: 700;
  color: #8C1D40; /* ASU Maroon */
  text-decoration: none;
  margin-bottom: 1rem;
  padding-top: 0.75rem;
}  
}
.subsection-title {
  font-size: 1rem;
  font-weight: 700;
  margin-bottom: 1rem;
  padding-top: 0.75rem;
}

.pub-list {
  display: grid;
  gap: 1.25rem;
}

.pub-item {
  display: grid;
  grid-template-columns: 120px 1fr;
  gap: 1rem;
  align-items: start;
  padding: 0.75rem 0;
  border-bottom: 1px solid rgba(0,0,0,0.04);
}

.thumb {
  margin: 0;
  width: 120px;
  height: auto;
}
.thumb img {
  width: 100%;
  height: 100%;
  border-radius: 4px;
  object-fit: cover;
  border: 1px solid #eee;
}

.pub-title {
  margin: 0 0 0.35rem 0;
  font-size: 1.05rem;
}
.pub-title a {
  color: #8C1D40; /* ASU Maroon */
  text-decoration: none;
  font-weight: 700;
}
.pub-title a:hover { text-decoration: underline; }

.pub-meta {
  font-size: 0.9rem;
  color: #222;
  margin-bottom: 0.4rem;
}
.pub-meta .authors { display: block; color: ##000000; font-weight: 500; }
.pub-meta .venue { display: block; color: inherit; font-weight: 700; margin-top: 0.15rem } 

.pub-abstract {
  margin: 0.5rem 0 0.6rem 0;
  color: #555;
  line-height: 1.45;
  text-align: justify
}

.pub-links {
  display: flex;
  gap: 0.5rem;
}
.btn {
  display: inline-block;
  padding: 0.32rem 0.6rem;
  border-radius: 6px;
  font-size: 0.85rem;
  text-decoration: none;
  border: 1px solid transparent;
  box-sizing: border-box;
}
.btn-paper {
  background: #FFC627;
  color: #000000;
  border-color: rgba(11,99,214,0.12);
}
.btn-site {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}

.btn-code {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}
.btn-data {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}
.btn-slides {
  background: #FFC627;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}

.google-scholar-link {
  text-align: center;
  margin: 0.5rem 0 0rem;
}

.google-scholar-link a {
  display: inline-block;
  background-color: #FFC627; /* ASU Gold */
  color: #000000;
  text-decoration: none;
  padding: 0.3rem 0.8rem;
  border-radius: 5px;
  font-weight: normal;
  transition: background-color 0.3s ease;
}

.google-scholar-link a:hover {
  background-color: #FFC627;
}

/* Responsive fallback */
@media (max-width: 740px) {
  .pub-item { grid-template-columns: 1fr; }
  .thumb { width: 100%; }
}
</style>
