---
layout: default
title: Publications
permalink: /publication/
---
<!-- selected-publications.html -->
<section class="selected-publications">
  <h2 class="section-title">Complex Data (Structured Data)</h2>

  <div class="pub-list">

    

    <!-- Publication item -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/MapIQ_poster_placeholder.png" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2507.11625" target="_blank" rel="noopener">MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">
            V Srivastava, F Lei, S Mukhopadhyay, V Gupta, R Maciejewski
          </span>
          <span class="venue">
              Published at <a href="https://colmweb.org/" target="_blank" rel="noopener">COLM 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          Recent advancements in multimodal large language models (MLLMs) have
 driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to
 visual question answering with maps (Map-VQA). However, Map-VQA
 research has primarily focused on choropleth maps, which cover only a
 limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising
 14,706 question-answer pairs across three map types—choropleth maps,
 cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another
 and a human baseline. An additional experiment...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2507.11625" target="_blank" rel="noopener">paper</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="{{ site.baseurl }}/poster_docs/TabXEval_poster_placeholder.png" alt="paper thumbnail">

      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://coral-lab-asu.github.io/tabxeval/" target="_blank" rel="noopener">TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"></span>
          Vihang Pancholi, Jainit Sushil Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta
          <span class="venue"> 
          Published at <a href="https://2025.aclweb.org/">ACL 2025</a>
          </span>
         
        </div>

        <p class="pub-abstract">
 Evaluating tables qualitatively and quantita
tively poses a significant challenge, as standard
 metrics often overlook subtle structural and
 content-level discrepancies. To address this,
 we propose a rubric-based evaluation frame
work that integrates multi-level structural de
scriptors with fine-grained contextual signals,
 enabling more precise and consistent table
 comparison. Building on this, we introduce
 TabXEval, an eXhaustive and eXplainable
 two-phase evaluation framework. TabXEval first aligns reference and predicted...
          
        </p>

        <div class="pub-links">
            
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-acl.1176.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/tabxeval/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TabXEval">code</a>
        </div>
      </div>
    </article>

    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-long.1460/" target="_blank" rel="noopener">Map&Make: Schema Guided Text to Table Generation</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Naman Ahuja, Fenil Bardoliya, Chitta Baral, Vivek Gupta</span>
          <span class="venue">
          Published at <a href="https://2025.aclweb.org/">ACL 2025</a>
          </span>
        </div>

        <p class="pub-abstract">
          Transforming dense, unstructured text into interpretable tables—commonly referred to as Text-to-Table generation—is a key task in information extraction. Existing methods often overlook what complex information to extract and how to infer it from text. We present Map&Make, a versatile approach that decomposes text into atomic propositions to infer latent schemas, which are then used to generate tables capturing both qualitative nuances and quantitative facts. We evaluate...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-long.1460/">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/map-make/">website</a>
          <a class="btn btn-code" href="https://github.com/coral-lab-asu/map-make/">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-demo.62/" target="_blank" rel="noopener">PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-demo.62/">paper</a>
          <a class="btn btn-site" href="https://project-praise.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/project-PRAISE/system-demo/">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-acl.1022/" target="_blank" rel="noopener">LLM-Symbolic Integration for Robust Temporal Tabular Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Atharv Kulkarni, Kushagra Dixit, Vivek Srikumar, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Temporal tabular question answering presents a significant challenge for Large Language Models (LLMs), requiring robust reasoning over structured data—a task where traditional prompting methods often fall short. These methods face challenges such as memorization, sensitivity to table size, and reduced performance on complex queries. To overcome these limitations, we introduce TEMPTABQA-C, a synthetic dataset designed for systematic and controlled evaluations, alongside a symbolic intermediate representation that transforms tables into database schemas. This structured approach allows LLMs to generate and execute SQL queries, enhancing generalization and mitigating biases. By incorporating...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-acl.1022/">paper</a>
          <a class="btn btn-site" href="https://coral-lab-asu.github.io/llm_symbolic/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/TEMPTABQA-C">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.acl-long.1439.pdf" target="_blank" rel="noopener">GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.aclweb.org/">ACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Publicly significant images from events carry
 valuable contextual information with applications in domains such as journalism and education. However, existing methodologies of ten struggle to accurately extract this contextual relevance from images. To address
 this challenge, we introduce GETREASON(Geospatial Event Temporal Reasoning), a
 framework designed to go beyond surface
level image descriptions and infer deeper con
textual meaning. We hypothesize that ex
tracting global event, temporal, and geospa
tial information from an image enables a
 more accurate understanding of its contex
tual significance. We also introduce a new
 metric GREAT (Geospatial, Reasoning and
 Event Accuracy with Temporal alignment) for
 a reasoning capturing evaluation. Our lay
ered multi-agentic approach, evaluated...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.acl-long.1439.pdf">paper</a>
          <a class="btn btn-site" href="https://coral-asu.github.io/getreason/">website</a>
          <a class="btn btn-code" href="https://github.com/CoRAL-ASU/getreason">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/abs/2409.00255" target="_blank" rel="noopener">MAPWise: Evaluating Vision-Language Models for Advanced Map Queries</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Manish Shrivastava, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
            
             Vision-language models (VLMs) excel at tasks
 requiring joint understanding of visual and lin
guistic information. A particularly promising
 yet under-explored application for these models
 lies in answering questions based on various
 kinds of maps. This study investigates the
 efficacy of VLMs in answering questions based
 on choropleth maps, which are widely used for
 data analysis and representation. To facilitate
 and encourage research in this area, we intro
duce a novel map-based question-answering
 benchmark, consisting of maps from three
 geographical regions (United States, India,
 China), each containing 1000 questions. Our
 benchmark...
            
            
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/abs/2409.00255">paper</a>
          <a class="btn btn-site" href="https://map-wise.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/abhishekrajgaria/Mapwise">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://example.com/paper2" target="_blank" rel="noopener">Leveraging LLM for Synchronizing Information Across Multilingual Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.329/">paper</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.naacl-long.332.pdf" target="_blank" rel="noopener">TRANSIENT TABLES: Evaluating LLMs’ Reasoning on Temporally Evolving Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Humans continuously make new discoveries,
 and understanding temporal sequence of events
 leading to these breakthroughs is essential for
 advancing science and society. This ability to
 reason over time allows us to identify future
 steps and understand the effects of financial
 and political decisions on our lives. However,
 large language models (LLMs) are typically
 trained on static datasets, limiting their ability
 to perform effective temporal reasoning. To
 assess the temporal reasoning capabilities of
 LLMs, we present the TRANSIENTTABLES
 dataset, which comprises 3,971 questions de
rived from over 14,000 tables, spanning 1,238
 entities across multiple time periods. We in
troduce a template-based question-generation
 pipeline that harnesses LLMs to refine both
 templates and questions. Additionally, we es
tablish baseline...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.naacl-long.332.pdf">paper</a>
          <a class="btn btn-site" href="https://transienttables.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/harsh1399/TransientTables">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/abs/2407.05952" target="_blank" rel="noopener">H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Nikhil Abhyankar, Vivek Gupta, Dan Roth, Chandan Reddy</span>
          <span class="venue">
              Published at <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
          Tabular reasoning involves interpreting natural language queries about tabular data, which presents a unique challenge of combining language understanding with structured data analysis. Existing methods employ either textual reasoning, which excels in semantic interpretation but struggles with mathematical
 operations, or symbolic reasoning, which handles computations well but lacks semantic understanding. This paper introduces a novel
 algorithm H-STAR that integrates both symbolic and semantic (textual) approaches in a
 two-stage process to address these limitations.
 H-STAR employs: (1) step-wise table extraction using ‘multi-view’ column retrieval followed by row extraction, and (2) adaptive reasoning that adapts reasoning strategies based on question types, utilizing semantic reasoning for direct lookup and complex lexical queries while augmenting textual reasoning with symbolic reasoning support for quantitative and logical tasks. Our extensive...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/abs/2407.05952">paper</a>
          <a class="btn btn-site" href="https://github.com/nikhilsab/H-STAR">website</a>
          <a class="btn btn-code" href="https://hstar-llm.github.io/">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2025.findings-naacl.278.pdf" target="_blank" rel="noopener">Enhancing Temporal Understanding in LLMs for Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Irwin Deng, Kushagra Dixit, Dan Roth, Vivek Gupta</span>
          
          <span class="venue">
              Published at <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
        </div>

        <p class="pub-abstract">
           Temporal reasoning over tabular data presents
 substantial challenges for large language mod
els (LLMs), as evidenced by recent research. In
 this study, we conduct a comprehensive analy
sis of temporal datasets to pinpoint the specific
 limitations of LLMs. Our investigation leads
 to enhancements in TempTabQA, a benchmark
 specifically designed for tabular temporal ques
tion answering. We provide critical insights
 for enhancing LLM performance in temporal
 reasoning tasks with tabular data. Furthermore,
 we introduce a novel approach, C.L.E.A.R to
 strengthen LLM capabilities in this domain.
 Our findings demonstrate that our method im
proves evidence-based reasoning across various
 models. Additionally, our experimental...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2025.findings-naacl.278.pdf">paper</a>
          <a class="btn btn-site" href="https://clear-prompting.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/clear-temptabqa/clear_temptabqa">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://arxiv.org/pdf/2407.10380" target="_blank" rel="noopener">NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Pranshu Pandya, Vatsal Gupta, Agney S Talwarr, Tushar Kataria, Dan Roth, Vivek Gupta</span>
          <span class="venue">
              Published at <a href="https://2025.naacl.org/">NAACL 2025</a>
              </span>
          
        </div>

        <p class="pub-abstract">
           Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand
 the ability to quickly reason, decipher, and
 evaluate patterns both textually and spatially.
 Due to extensive training on vast amounts of
 human-curated data, large language models
 (LLMs) and vision language models (VLMs)
 excel in common-sense reasoning tasks, but
 still struggle with more complex reasoning
 that demands deeper cognitive understanding.
 We introduce NTSEBENCH, a new dataset designed to evaluate cognitive multimodal reasoning and problem-solving skills of large models. The dataset contains 2,728 multiple-choice
 questions, accompanied by a total of 4,642 images, spanning 26 categories....
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://arxiv.org/pdf/2407.10380">paper</a>
          <a class="btn btn-site" href="https://ntsebench.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/NTSEBench/NTSEBench">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.emnlp-main.1237.pdf" target="_blank" rel="noopener">Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Vatsal Gupta, Pranshu Pandya, Tushar Kataria, <b>Vivek Gupta</b>, Dan Roth <br></span>
          <span class="venue">
              Published at <a href="https://2024.emnlp.org/">EMNLP 2024</a>
             </span>
        </div>

        <p class="pub-abstract">
          Language models, characterized by their black
box nature, often hallucinate and display sensitivity to input perturbations, causing concerns
 about trust. To enhance trust, it is imperative
 to gain a comprehensive understanding of the
 model’s failure modes and develop effective
 strategies to improve their performance. In this
 study, we introduce a methodology designed
 to examine how input perturbations affect language models across various scales, including
 pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance
 the model’s robustness to input perturbations.
 Additionally, ...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.emnlp-main.1237.pdf">paper</a>
          <a class="btn btn-site" href="https://msin-infotabs.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/msin-infotabs/Evaluating-Concurrent-Robustness-of-Language-Models-Across-Diverse-Challenge-Sets">code</a>
          
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.973.pdf" target="_blank" rel="noopener">Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness</a>
        </h3>

        <div class="pub-meta">
          <span class="authors"><br>
  Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, <b>Vivek Gupta</b>, Dan Roth <br></span>
          
          <span class="venue">
          Published at <a href="https://2024.emnlp.org/">EMNLP 2024</a> (Finding)
          </span>
        </div>

        <p class="pub-abstract">
          Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally,...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.973/">paper</a>
          <a class="btn btn-site" href="https://robustcqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/RobustCQA/Robust-CQA">code</a>
        </div>
      </div>
    </article>
    <!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.822.pdf" target="_blank" rel="noopener">Knowledge-Aware Reasoning over Multimodal Semi-structured Tables</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, <b>Vivek Gupta</b>, Mohit Bansal, Dan Roth</span>
          <span class="venue">
              Published at <a href="https://2024.emnlp.org/">EMNLP 2024</a>
           </span>  
        </div>

        <p class="pub-abstract">
           Existing datasets for tabular question answer
ing typically focus exclusively on text within
 cells. However, real-world data is inherently
 multimodal, often blending images such as
 symbols, faces, icons, patterns, and charts
 with textual content in tables. With the evo
lution of AI models capable of multimodal
 reasoning, it is pertinent to assess their effi
cacy in handling such structured data. This
 study investigates whether current AI models
 can perform knowledge-aware reasoning on
 multimodal structured data. We explore their
 ability to reason on tables that integrate both
 images and text, introducing MMTABQA, a
 new dataset designed for this purpose. Our
 experiments...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.822.pdf">paper</a>
          <a class="btn btn-site" href="https://mmtabqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/MMTabQA/mmtabqa">code</a>
        </div>
      </div>
    </article>
    
  </div>
</section>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.822.pdf" target="_blank" rel="noopener">FlowVQA: Mapping Multimodal Logic in
 Visual Question Answering with Flowcharts</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, <b>Vivek Gupta</b>, Dan Roth</span>
          <span class="venue">
              Published at <a href="https://2024.aclweb.org/">ACL 2024</a>(Finding)
           </span>  
        </div>

        <p class="pub-abstract">
            Existing benchmarks for visual question an
swering lack in visual grounding and complex
ity, particularly in evaluating spatial reasoning
 skills. We introduce FlowVQA, a novel bench
mark aimed at assessing the capabilities of vi
sual question-answering multimodal language
 models in reasoning with flowcharts as visual
 contexts. FlowVQA comprises 2,272 carefully
 generated and human-verified flowchart images
 from three distinct content sources, along with
 22,413 diverse question-answer pairs, to test a
 spectrum of reasoning tasks, including informa
tion localization, decision-making, and logical
 progression. We conduct a thorough baseline
 evaluation on a suite of both open-source and
 proprietary multimodal language models using
 various strategies, followed by an analysis of
 directional bias. The results...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-emnlp.822.pdf">paper</a>
          <a class="btn btn-site" href="https://mmtabqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/MMTabQA/mmtabqa">code</a>
        </div>
      </div>
    </article>
    
  </div>
</section>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.231.pdf" target="_blank" rel="noopener">Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Pragya Srivastava, Manuj Malik, <b>Vivek Gupta</b>, Tanuja Ganu, Dan Roth</span>
          <span class="venue">
              Published at <a href="https://2024.aclweb.org/">ACL 2024</a>(Finding)
           </span>  
        </div>

        <p class="pub-abstract">
           Existing datasets for tabular question answer
ing typically focus exclusively on text within
 cells. However, real-world data is inherently
 multimodal, often blending images such as
 symbols, faces, icons, patterns, and charts
 with textual content in tables. With the evo
lution of AI models capable of multimodal
 reasoning, it is pertinent to assess their effi
cacy in handling such structured data. This
 study investigates whether current AI models
 can perform knowledge-aware reasoning on
 multimodal structured data. We explore their
 ability to reason on tables that integrate both
 images and text, introducing MMTABQA, a
 new dataset designed for this purpose. Our
 experiments...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.231.pdf">paper</a>
          <a class="btn btn-site" href="https://flowvqa.github.io/">website</a>
          <a class="btn btn-code" href="https://github.com/flowvqa/flowvqa">code</a>
        </div>
      </div>
    </article>
    
  </div>
</section>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-acl.828.pdf" target="_blank" rel="noopener">ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World Chart Images</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Mubashara Akhtar, Nikesh Subedi, <b>Vivek Gupta</b>, Sahar Tahmasebi, Oana Cocarascu, Elena Simperl</span>
          <span class="venue">
              Published at <a href="https://2024.aclweb.org/">ACL 2024</a>(Finding)
           </span>  
        </div>

        <p class="pub-abstract">
           Whilst fact verification has attracted substan
tial interest in the natural language processing
 community, verifying misinforming statements
 against data visualizations such as charts has
 so far been overlooked. Charts are commonly
 used in the real-world to summarize and com
municate key information, but they can also
 be easily misused to spread misinformation
 and promote certain agendas. In this paper,
 we introduce ChartCheck, a novel, large-scale
 dataset for explainable fact-checking against
 Chart:
 Evidence
 real-world charts, consisting of 1.7k charts and
 10.5k human-written claims and explanations.
 We systematically...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.findings-acl.828.pdf">paper</a>
          <a class="btn btn-code" href="https://github.com/mubasharaak/ChartCheck">code</a>
        </div>
      </div>
    </article>
    
  </div>
</section>

<!-- /Publication item -->

    <!-- Repeat other pub-items as needed -->
    <article class="pub-item">
      <figure class="thumb">
        <img src="/assets/images/thumb-example-2.jpg" alt="paper thumbnail">
      </figure>

      <div class="pub-body">
        <h3 class="pub-title">
          <a href="https://aclanthology.org/2024.findings-emnlp.822.pdf" target="_blank" rel="noopener">Enhancing Question Answering on Charts Through Effective Pre-training Tasks</a>
        </h3>

        <div class="pub-meta">
          <span class="authors">Ashim Gupta, <b>Vivek Gupta</b>, Shuo Zhang, Yujie He, Ning Zhang, Shalin Shah </span>
          <span class="venue">
              Published at <a href="https://blackboxnlp.github.io/">BlackboxNLP 2024</a>
              
           </span>  
        </div>

        <p class="pub-abstract">
            To completely understand a document, the use
 of textual information is not enough. Under
standing visual cues, such as layouts and charts,
 is also required. While the current state-of
the-art approaches for document understanding
 (both OCR-based and OCR-free) work well,
 we have not found any other works conduct
ing a thorough analysis of their capabilities and
 limitations. Therefore, in this work, we ad
dress the limitation of current VisualQA mod
els when applied to charts and plots. To in
vestigate shortcomings of the state-of-the-art
 models, we conduct a comprehensive behav
ioral analysis, using ChartQA as a case study.
 Our findings ...
        </p>

        <div class="pub-links">
          <a class="btn btn-paper" href="https://aclanthology.org/2024.blackboxnlp-1.11.pdf">paper</a>
        </div>
      </div>
    </article>
    
  </div>
</section>


<style>
/* Basic styling — tweak to match fonts/colors of your site */
.selected-publications {
  max-width: 980px;
  margin: 2rem auto;
  padding: 0 1rem;
  font-family: "Helvetica Neue", Arial, sans-serif;
}

.section-title {
  font-size: 1.25rem;
  font-weight: 700;
  margin-bottom: 1rem;
  border-top: 2px solid #e6e6e6;
  padding-top: 0.75rem;
}

.pub-list {
  display: grid;
  gap: 1.25rem;
}

.pub-item {
  display: grid;
  grid-template-columns: 120px 1fr;
  gap: 1rem;
  align-items: start;
  padding: 0.75rem 0;
  border-bottom: 1px solid rgba(0,0,0,0.04);
}

.thumb {
  margin: 0;
  width: 120px;
  height: auto;
}
.thumb img {
  width: 100%;
  height: auto;
  border-radius: 4px;
  object-fit: cover;
  border: 1px solid #eee;
}

.pub-title {
  margin: 0 0 0.35rem 0;
  font-size: 1.05rem;
}
.pub-title a {
  color: #800000; /* blue link like screenshot */
  text-decoration: none;
  font-weight: 700;
}
.pub-title a:hover { text-decoration: underline; }

.pub-meta {
  font-size: 0.9rem;
  color: #222;
  margin-bottom: 0.4rem;
}
.pub-meta .authors { display: block; color: ##000000; font-weight: 500; }
.pub-meta .venue { display: block; color: inherit; font-weight: 700; margin-top: 0.15rem } 

.pub-abstract {
  margin: 0.5rem 0 0.6rem 0;
  color: #555;
  line-height: 1.45;
}

.pub-links {
  display: flex;
  gap: 0.5rem;
}
.btn {
  display: inline-block;
  padding: 0.32rem 0.6rem;
  border-radius: 6px;
  font-size: 0.85rem;
  text-decoration: none;
  border: 1px solid transparent;
  box-sizing: border-box;
}
.btn-paper {
  background: #FFD700;
  color: #000000;
  border-color: rgba(11,99,214,0.12);
}
.btn-site {
  background: #FFD700;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}

.btn-code {
  background: #FFD700;
  color: #000000;
  border-color: rgba(110,42,168,0.08);
}
/* Responsive fallback */
@media (max-width: 740px) {
  .pub-item { grid-template-columns: 1fr; }
  .thumb { width: 100%; }
}
</style>
